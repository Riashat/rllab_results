2017-07-31 15:59:58.298210 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] observation space: Box(11,)
2017-07-31 15:59:58.299150 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] action space: Box(3,)
2017-07-31 15:59:58.838695 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] Populating workers...
2017-07-31 15:59:58.839220 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] Populated
2017-07-31 16:00:01.760316 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #0 | Training started
2017-07-31 16:00:04.416292 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #0 | Training finished
2017-07-31 16:00:04.416583 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #0 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:04.417070 EDT | -------------------  ---
2017-07-31 16:00:04.417251 EDT | Oracle Interactions  407
2017-07-31 16:00:04.417414 EDT | Agent Interactions   593
2017-07-31 16:00:04.417575 EDT | -------------------  ---
2017-07-31 16:00:04.417830 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #1 | Training started
2017-07-31 16:00:07.108969 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #1 | Training finished
2017-07-31 16:00:07.109817 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #1 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:07.110412 EDT | -------------------  ----
2017-07-31 16:00:07.110687 EDT | Oracle Interactions   909
2017-07-31 16:00:07.110944 EDT | Agent Interactions   1091
2017-07-31 16:00:07.111194 EDT | -------------------  ----
2017-07-31 16:00:07.111539 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #2 | Training started
2017-07-31 16:00:09.724619 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #2 | Training finished
2017-07-31 16:00:09.725098 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #2 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:09.725777 EDT | -------------------  ----
2017-07-31 16:00:09.726091 EDT | Oracle Interactions  1360
2017-07-31 16:00:09.726409 EDT | Agent Interactions   1640
2017-07-31 16:00:09.726699 EDT | -------------------  ----
2017-07-31 16:00:09.727098 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #3 | Training started
2017-07-31 16:00:12.217741 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #3 | Training finished
2017-07-31 16:00:12.218522 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #3 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:12.219064 EDT | -------------------  ----
2017-07-31 16:00:12.219344 EDT | Oracle Interactions  1844
2017-07-31 16:00:12.219603 EDT | Agent Interactions   2156
2017-07-31 16:00:12.219859 EDT | -------------------  ----
2017-07-31 16:00:12.220176 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #4 | Training started
2017-07-31 16:00:14.483032 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #4 | Training finished
2017-07-31 16:00:14.483403 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #4 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:14.483941 EDT | -------------------  ----
2017-07-31 16:00:14.484214 EDT | Oracle Interactions  2315
2017-07-31 16:00:14.484469 EDT | Agent Interactions   2685
2017-07-31 16:00:14.484718 EDT | -------------------  ----
2017-07-31 16:00:14.485041 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #5 | Training started
2017-07-31 16:00:16.781912 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #5 | Training finished
2017-07-31 16:00:16.782278 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #5 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:16.782799 EDT | -------------------  ----
2017-07-31 16:00:16.783059 EDT | Oracle Interactions  2806
2017-07-31 16:00:16.783302 EDT | Agent Interactions   3194
2017-07-31 16:00:16.783540 EDT | -------------------  ----
2017-07-31 16:00:16.783850 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #6 | Training started
2017-07-31 16:00:19.316015 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #6 | Training finished
2017-07-31 16:00:19.316525 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #6 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:19.317276 EDT | -------------------  ----
2017-07-31 16:00:19.317655 EDT | Oracle Interactions  3257
2017-07-31 16:00:19.318053 EDT | Agent Interactions   3743
2017-07-31 16:00:19.318474 EDT | -------------------  ----
2017-07-31 16:00:19.318967 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #7 | Training started
2017-07-31 16:00:21.682619 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #7 | Training finished
2017-07-31 16:00:21.683499 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #7 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:21.683989 EDT | -------------------  ----
2017-07-31 16:00:21.684187 EDT | Oracle Interactions  3751
2017-07-31 16:00:21.684364 EDT | Agent Interactions   4249
2017-07-31 16:00:21.684530 EDT | -------------------  ----
2017-07-31 16:00:21.684833 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #8 | Training started
2017-07-31 16:00:31.125008 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #8 | Training finished
2017-07-31 16:00:31.125767 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #8 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:00:31.126433 EDT | -------------------  ----
2017-07-31 16:00:31.126747 EDT | Oracle Interactions  4218
2017-07-31 16:00:31.127051 EDT | Agent Interactions   4782
2017-07-31 16:00:31.127339 EDT | -------------------  ----
2017-07-31 16:00:31.127713 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #9 | Training started
2017-07-31 16:00:34.057222 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #9 | Training finished
2017-07-31 16:00:34.057596 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #9 | Trained qf 1 steps, policy 1 steps
2017-07-31 16:00:34.057878 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #9 | Collecting samples for evaluation
2017-07-31 16:00:53.849557 EDT | -----------------------  -------------
2017-07-31 16:00:53.851060 EDT | Oracle Interactions      4693
2017-07-31 16:00:53.851358 EDT | Agent Interactions       5307
2017-07-31 16:00:53.851627 EDT | Epoch                       9
2017-07-31 16:00:53.851891 EDT | Iteration                   9
2017-07-31 16:00:53.852143 EDT | AverageReturn              25.4216
2017-07-31 16:00:53.852400 EDT | StdReturn                   0.321732
2017-07-31 16:00:53.852654 EDT | MaxReturn                  26.1958
2017-07-31 16:00:53.852906 EDT | MinReturn                  24.7016
2017-07-31 16:00:53.853155 EDT | AverageEsReturn            16.3967
2017-07-31 16:00:53.853404 EDT | StdEsReturn                14.2814
2017-07-31 16:00:53.853650 EDT | MaxEsReturn                99.6985
2017-07-31 16:00:53.853896 EDT | MinEsReturn                 2.22407
2017-07-31 16:00:53.854146 EDT | AverageDiscountedReturn    21.8019
2017-07-31 16:00:53.854396 EDT | AverageQLoss                0.649878
2017-07-31 16:00:53.854564 EDT | AveragePolicySurr          -0.287193
2017-07-31 16:00:53.854722 EDT | AverageQ                    0.291994
2017-07-31 16:00:53.854873 EDT | AverageAbsQ                 0.300461
2017-07-31 16:00:53.855030 EDT | AverageY                    0.879255
2017-07-31 16:00:53.855182 EDT | AverageAbsY                 0.916596
2017-07-31 16:00:53.855334 EDT | AverageAbsQYDiff            0.728357
2017-07-31 16:00:53.855489 EDT | AverageAction               0.00272409
2017-07-31 16:00:53.855640 EDT | PolicyRegParamNorm         13.3334
2017-07-31 16:00:53.855792 EDT | QFunRegParamNorm           11.0996
2017-07-31 16:00:53.855943 EDT | -----------------------  -------------
2017-07-31 16:00:53.856237 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #10 | Training started
2017-07-31 16:01:23.145156 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #10 | Training finished
2017-07-31 16:01:23.146161 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #10 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:01:23.146391 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #10 | Collecting samples for evaluation
2017-07-31 16:01:42.359938 EDT | -----------------------  ------------
2017-07-31 16:01:42.361048 EDT | Oracle Interactions      5681
2017-07-31 16:01:42.361334 EDT | Agent Interactions       5319
2017-07-31 16:01:42.361601 EDT | Epoch                      10
2017-07-31 16:01:42.361860 EDT | Iteration                  10
2017-07-31 16:01:42.362111 EDT | AverageReturn               5.83817
2017-07-31 16:01:42.362405 EDT | StdReturn                   0.0617014
2017-07-31 16:01:42.362654 EDT | MaxReturn                   6.01726
2017-07-31 16:01:42.362903 EDT | MinReturn                   5.69673
2017-07-31 16:01:42.363150 EDT | AverageEsReturn            27.4778
2017-07-31 16:01:42.363396 EDT | StdEsReturn                 7.65326
2017-07-31 16:01:42.363645 EDT | MaxEsReturn                51.774
2017-07-31 16:01:42.363889 EDT | MinEsReturn                 6.07567
2017-07-31 16:01:42.364139 EDT | AverageDiscountedReturn     5.67925
2017-07-31 16:01:42.364389 EDT | AverageQLoss                0.0215909
2017-07-31 16:01:42.364634 EDT | AveragePolicySurr          -1.11072
2017-07-31 16:01:42.364883 EDT | AverageQ                    1.23192
2017-07-31 16:01:42.365128 EDT | AverageAbsQ                 1.25732
2017-07-31 16:01:42.365372 EDT | AverageY                    1.236
2017-07-31 16:01:42.365617 EDT | AverageAbsY                 1.26487
2017-07-31 16:01:42.365864 EDT | AverageAbsQYDiff            0.0839201
2017-07-31 16:01:42.366109 EDT | AverageAction               0.999986
2017-07-31 16:01:42.366395 EDT | PolicyRegParamNorm         13.9384
2017-07-31 16:01:42.366657 EDT | QFunRegParamNorm           11.4969
2017-07-31 16:01:42.366915 EDT | -----------------------  ------------
2017-07-31 16:01:42.367405 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #11 | Training started
2017-07-31 16:02:12.165279 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #11 | Training finished
2017-07-31 16:02:12.166096 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #11 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:02:12.166382 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #11 | Collecting samples for evaluation
2017-07-31 16:02:30.356444 EDT | -----------------------  ------------
2017-07-31 16:02:30.357649 EDT | Oracle Interactions      6681
2017-07-31 16:02:30.357919 EDT | Agent Interactions       5319
2017-07-31 16:02:30.358169 EDT | Epoch                      11
2017-07-31 16:02:30.358408 EDT | Iteration                  11
2017-07-31 16:02:30.358642 EDT | AverageReturn              38.7556
2017-07-31 16:02:30.358875 EDT | StdReturn                   0.972188
2017-07-31 16:02:30.359105 EDT | MaxReturn                  40.7742
2017-07-31 16:02:30.359336 EDT | MinReturn                  36.5856
2017-07-31 16:02:30.359565 EDT | AverageEsReturn            27.0399
2017-07-31 16:02:30.359794 EDT | StdEsReturn                 0.655249
2017-07-31 16:02:30.360023 EDT | MaxEsReturn                28.3152
2017-07-31 16:02:30.360251 EDT | MinEsReturn                25.6703
2017-07-31 16:02:30.360480 EDT | AverageDiscountedReturn    34.4465
2017-07-31 16:02:30.360707 EDT | AverageQLoss                0.0234893
2017-07-31 16:02:30.360934 EDT | AveragePolicySurr          -1.44871
2017-07-31 16:02:30.361162 EDT | AverageQ                    1.86215
2017-07-31 16:02:30.361390 EDT | AverageAbsQ                 1.89456
2017-07-31 16:02:30.361618 EDT | AverageY                    1.86326
2017-07-31 16:02:30.361846 EDT | AverageAbsY                 1.89651
2017-07-31 16:02:30.362073 EDT | AverageAbsQYDiff            0.0848859
2017-07-31 16:02:30.363696 EDT | AverageAction               1
2017-07-31 16:02:30.364120 EDT | PolicyRegParamNorm         14.0737
2017-07-31 16:02:30.364543 EDT | QFunRegParamNorm           12.2074
2017-07-31 16:02:30.364975 EDT | -----------------------  ------------
2017-07-31 16:02:30.365590 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #12 | Training started
2017-07-31 16:03:00.972361 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #12 | Training finished
2017-07-31 16:03:00.973309 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #12 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:03:00.973565 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #12 | Collecting samples for evaluation
2017-07-31 16:03:20.502710 EDT | -----------------------  ------------
2017-07-31 16:03:20.506561 EDT | Oracle Interactions      7681
2017-07-31 16:03:20.506890 EDT | Agent Interactions       5319
2017-07-31 16:03:20.507182 EDT | Epoch                      12
2017-07-31 16:03:20.507475 EDT | Iteration                  12
2017-07-31 16:03:20.507758 EDT | AverageReturn              38.8058
2017-07-31 16:03:20.508040 EDT | StdReturn                   0.953829
2017-07-31 16:03:20.508323 EDT | MaxReturn                  41.8291
2017-07-31 16:03:20.508597 EDT | MinReturn                  36.6296
2017-07-31 16:03:20.508873 EDT | AverageEsReturn            27.1956
2017-07-31 16:03:20.509144 EDT | StdEsReturn                 0.445312
2017-07-31 16:03:20.509419 EDT | MaxEsReturn                28.3627
2017-07-31 16:03:20.509691 EDT | MinEsReturn                26.2808
2017-07-31 16:03:20.509967 EDT | AverageDiscountedReturn    34.4841
2017-07-31 16:03:20.510300 EDT | AverageQLoss                0.0450974
2017-07-31 16:03:20.510636 EDT | AveragePolicySurr          -1.95859
2017-07-31 16:03:20.510944 EDT | AverageQ                    2.65945
2017-07-31 16:03:20.511277 EDT | AverageAbsQ                 2.68714
2017-07-31 16:03:20.511668 EDT | AverageY                    2.66054
2017-07-31 16:03:20.512032 EDT | AverageAbsY                 2.68811
2017-07-31 16:03:20.512383 EDT | AverageAbsQYDiff            0.107823
2017-07-31 16:03:20.512739 EDT | AverageAction               1
2017-07-31 16:03:20.513102 EDT | PolicyRegParamNorm         14.0818
2017-07-31 16:03:20.513434 EDT | QFunRegParamNorm           13.263
2017-07-31 16:03:20.513744 EDT | -----------------------  ------------
2017-07-31 16:03:20.514343 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #13 | Training started
2017-07-31 16:03:50.553822 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #13 | Training finished
2017-07-31 16:03:50.555863 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #13 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:03:50.556182 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #13 | Collecting samples for evaluation
2017-07-31 16:04:09.149892 EDT | -----------------------  -----------
2017-07-31 16:04:09.151273 EDT | Oracle Interactions      8681
2017-07-31 16:04:09.151594 EDT | Agent Interactions       5319
2017-07-31 16:04:09.151888 EDT | Epoch                      13
2017-07-31 16:04:09.152181 EDT | Iteration                  13
2017-07-31 16:04:09.152466 EDT | AverageReturn              38.8338
2017-07-31 16:04:09.152752 EDT | StdReturn                   0.927834
2017-07-31 16:04:09.153033 EDT | MaxReturn                  40.7333
2017-07-31 16:04:09.153314 EDT | MinReturn                  36.6665
2017-07-31 16:04:09.153599 EDT | AverageEsReturn            27.1049
2017-07-31 16:04:09.153885 EDT | StdEsReturn                 0.642498
2017-07-31 16:04:09.154175 EDT | MaxEsReturn                28.6176
2017-07-31 16:04:09.154460 EDT | MinEsReturn                26.1523
2017-07-31 16:04:09.154740 EDT | AverageDiscountedReturn    34.5084
2017-07-31 16:04:09.155025 EDT | AverageQLoss                0.064889
2017-07-31 16:04:09.155311 EDT | AveragePolicySurr          -2.46407
2017-07-31 16:04:09.155590 EDT | AverageQ                    3.5308
2017-07-31 16:04:09.155869 EDT | AverageAbsQ                 3.55622
2017-07-31 16:04:09.156148 EDT | AverageY                    3.53213
2017-07-31 16:04:09.156428 EDT | AverageAbsY                 3.55756
2017-07-31 16:04:09.156708 EDT | AverageAbsQYDiff            0.12167
2017-07-31 16:04:09.156987 EDT | AverageAction               1
2017-07-31 16:04:09.157266 EDT | PolicyRegParamNorm         14.0922
2017-07-31 16:04:09.157566 EDT | QFunRegParamNorm           14.447
2017-07-31 16:04:09.157845 EDT | -----------------------  -----------
2017-07-31 16:04:09.160056 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #14 | Training started
2017-07-31 16:04:39.565072 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #14 | Training finished
2017-07-31 16:04:39.566218 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #14 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:04:39.566680 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #14 | Collecting samples for evaluation
2017-07-31 16:04:56.928022 EDT | -----------------------  ------------
2017-07-31 16:04:56.929315 EDT | Oracle Interactions      9681
2017-07-31 16:04:56.929802 EDT | Agent Interactions       5319
2017-07-31 16:04:56.930314 EDT | Epoch                      14
2017-07-31 16:04:56.930778 EDT | Iteration                  14
2017-07-31 16:04:56.931231 EDT | AverageReturn              38.8503
2017-07-31 16:04:56.931711 EDT | StdReturn                   0.911701
2017-07-31 16:04:56.932165 EDT | MaxReturn                  40.7325
2017-07-31 16:04:56.932611 EDT | MinReturn                  36.7093
2017-07-31 16:04:56.933061 EDT | AverageEsReturn            27.2695
2017-07-31 16:04:56.933511 EDT | StdEsReturn                 0.61409
2017-07-31 16:04:56.933963 EDT | MaxEsReturn                28.2143
2017-07-31 16:04:56.934422 EDT | MinEsReturn                26.255
2017-07-31 16:04:56.934864 EDT | AverageDiscountedReturn    34.5208
2017-07-31 16:04:56.935310 EDT | AverageQLoss                0.0855632
2017-07-31 16:04:56.935894 EDT | AveragePolicySurr          -3.03755
2017-07-31 16:04:56.936838 EDT | AverageQ                    4.50767
2017-07-31 16:04:56.937519 EDT | AverageAbsQ                 4.53047
2017-07-31 16:04:56.938144 EDT | AverageY                    4.50876
2017-07-31 16:04:56.938770 EDT | AverageAbsY                 4.53132
2017-07-31 16:04:56.939454 EDT | AverageAbsQYDiff            0.135798
2017-07-31 16:04:56.940069 EDT | AverageAction               1
2017-07-31 16:04:56.940667 EDT | PolicyRegParamNorm         14.093
2017-07-31 16:04:56.941262 EDT | QFunRegParamNorm           15.8205
2017-07-31 16:04:56.941843 EDT | -----------------------  ------------
2017-07-31 16:04:56.942637 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #15 | Training started
2017-07-31 16:05:28.148592 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #15 | Training finished
2017-07-31 16:05:28.152172 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #15 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:05:28.152470 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #15 | Collecting samples for evaluation
2017-07-31 16:05:46.158925 EDT | -----------------------  ------------
2017-07-31 16:05:46.159938 EDT | Oracle Interactions      10681
2017-07-31 16:05:46.160134 EDT | Agent Interactions        5319
2017-07-31 16:05:46.160308 EDT | Epoch                       15
2017-07-31 16:05:46.160476 EDT | Iteration                   15
2017-07-31 16:05:46.160641 EDT | AverageReturn               38.7853
2017-07-31 16:05:46.160801 EDT | StdReturn                    0.924018
2017-07-31 16:05:46.160958 EDT | MaxReturn                   40.7399
2017-07-31 16:05:46.161123 EDT | MinReturn                   36.7112
2017-07-31 16:05:46.161289 EDT | AverageEsReturn             27.1839
2017-07-31 16:05:46.161498 EDT | StdEsReturn                  0.414614
2017-07-31 16:05:46.161663 EDT | MaxEsReturn                 27.9309
2017-07-31 16:05:46.161820 EDT | MinEsReturn                 26.4224
2017-07-31 16:05:46.161983 EDT | AverageDiscountedReturn     34.4699
2017-07-31 16:05:46.162149 EDT | AverageQLoss                 0.112828
2017-07-31 16:05:46.162313 EDT | AveragePolicySurr           -3.71376
2017-07-31 16:05:46.162474 EDT | AverageQ                     5.61901
2017-07-31 16:05:46.162686 EDT | AverageAbsQ                  5.64041
2017-07-31 16:05:46.162854 EDT | AverageY                     5.62068
2017-07-31 16:05:46.163013 EDT | AverageAbsY                  5.64193
2017-07-31 16:05:46.163173 EDT | AverageAbsQYDiff             0.154692
2017-07-31 16:05:46.163333 EDT | AverageAction                1
2017-07-31 16:05:46.163493 EDT | PolicyRegParamNorm          14.0971
2017-07-31 16:05:46.163656 EDT | QFunRegParamNorm            17.0626
2017-07-31 16:05:46.163806 EDT | -----------------------  ------------
2017-07-31 16:05:46.164142 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #16 | Training started
2017-07-31 16:06:19.069032 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #16 | Training finished
2017-07-31 16:06:19.353493 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #16 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:06:19.354050 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #16 | Collecting samples for evaluation
2017-07-31 16:06:37.720352 EDT | -----------------------  ------------
2017-07-31 16:06:37.721016 EDT | Oracle Interactions      11681
2017-07-31 16:06:37.721241 EDT | Agent Interactions        5319
2017-07-31 16:06:37.721556 EDT | Epoch                       16
2017-07-31 16:06:37.721729 EDT | Iteration                   16
2017-07-31 16:06:37.721906 EDT | AverageReturn               38.8044
2017-07-31 16:06:37.722071 EDT | StdReturn                    0.970162
2017-07-31 16:06:37.722315 EDT | MaxReturn                   40.697
2017-07-31 16:06:37.722536 EDT | MinReturn                   36.5797
2017-07-31 16:06:37.722718 EDT | AverageEsReturn             27.4396
2017-07-31 16:06:37.722974 EDT | StdEsReturn                  0.564982
2017-07-31 16:06:37.723138 EDT | MaxEsReturn                 28.1375
2017-07-31 16:06:37.723318 EDT | MinEsReturn                 26.3672
2017-07-31 16:06:37.723556 EDT | AverageDiscountedReturn     34.4839
2017-07-31 16:06:37.723766 EDT | AverageQLoss                 0.13158
2017-07-31 16:06:37.724081 EDT | AveragePolicySurr           -4.47731
2017-07-31 16:06:37.724354 EDT | AverageQ                     6.93999
2017-07-31 16:06:37.724532 EDT | AverageAbsQ                  6.95868
2017-07-31 16:06:37.724736 EDT | AverageY                     6.94145
2017-07-31 16:06:37.725025 EDT | AverageAbsY                  6.95871
2017-07-31 16:06:37.725194 EDT | AverageAbsQYDiff             0.17011
2017-07-31 16:06:37.725364 EDT | AverageAction                1
2017-07-31 16:06:37.725634 EDT | PolicyRegParamNorm          14.0988
2017-07-31 16:06:37.725796 EDT | QFunRegParamNorm            18.5202
2017-07-31 16:06:37.726003 EDT | -----------------------  ------------
2017-07-31 16:06:37.726465 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #17 | Training started
2017-07-31 16:07:09.116991 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #17 | Training finished
2017-07-31 16:07:09.117821 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #17 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:07:09.118084 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #17 | Collecting samples for evaluation
2017-07-31 16:07:27.445529 EDT | -----------------------  ------------
2017-07-31 16:07:27.447109 EDT | Oracle Interactions      12681
2017-07-31 16:07:27.447688 EDT | Agent Interactions        5319
2017-07-31 16:07:27.448226 EDT | Epoch                       17
2017-07-31 16:07:27.448732 EDT | Iteration                   17
2017-07-31 16:07:27.449257 EDT | AverageReturn               38.8271
2017-07-31 16:07:27.449754 EDT | StdReturn                    0.936805
2017-07-31 16:07:27.450288 EDT | MaxReturn                   40.6405
2017-07-31 16:07:27.450771 EDT | MinReturn                   36.751
2017-07-31 16:07:27.451250 EDT | AverageEsReturn             27.0918
2017-07-31 16:07:27.451767 EDT | StdEsReturn                  0.461775
2017-07-31 16:07:27.452241 EDT | MaxEsReturn                 28.0108
2017-07-31 16:07:27.452751 EDT | MinEsReturn                 26.2954
2017-07-31 16:07:27.453233 EDT | AverageDiscountedReturn     34.5022
2017-07-31 16:07:27.453745 EDT | AverageQLoss                 0.148451
2017-07-31 16:07:27.454235 EDT | AveragePolicySurr           -5.21676
2017-07-31 16:07:27.454739 EDT | AverageQ                     8.26812
2017-07-31 16:07:27.455221 EDT | AverageAbsQ                  8.28755
2017-07-31 16:07:27.455731 EDT | AverageY                     8.26996
2017-07-31 16:07:27.456211 EDT | AverageAbsY                  8.28776
2017-07-31 16:07:27.456709 EDT | AverageAbsQYDiff             0.188057
2017-07-31 16:07:27.457185 EDT | AverageAction                1
2017-07-31 16:07:27.457673 EDT | PolicyRegParamNorm          14.1055
2017-07-31 16:07:27.458191 EDT | QFunRegParamNorm            19.8342
2017-07-31 16:07:27.458491 EDT | -----------------------  ------------
2017-07-31 16:07:27.459007 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #18 | Training started
2017-07-31 16:07:59.034223 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #18 | Training finished
2017-07-31 16:07:59.035204 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #18 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:07:59.035544 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #18 | Collecting samples for evaluation
2017-07-31 16:08:18.258008 EDT | -----------------------  ------------
2017-07-31 16:08:18.261145 EDT | Oracle Interactions      13681
2017-07-31 16:08:18.261660 EDT | Agent Interactions        5319
2017-07-31 16:08:18.262142 EDT | Epoch                       18
2017-07-31 16:08:18.262609 EDT | Iteration                   18
2017-07-31 16:08:18.263069 EDT | AverageReturn               38.8038
2017-07-31 16:08:18.263521 EDT | StdReturn                    0.933067
2017-07-31 16:08:18.263967 EDT | MaxReturn                   40.6797
2017-07-31 16:08:18.264411 EDT | MinReturn                   36.7241
2017-07-31 16:08:18.264853 EDT | AverageEsReturn             27.003
2017-07-31 16:08:18.265293 EDT | StdEsReturn                  0.567074
2017-07-31 16:08:18.265731 EDT | MaxEsReturn                 28.1392
2017-07-31 16:08:18.266179 EDT | MinEsReturn                 26.1255
2017-07-31 16:08:18.266628 EDT | AverageDiscountedReturn     34.4844
2017-07-31 16:08:18.267071 EDT | AverageQLoss                 0.175768
2017-07-31 16:08:18.267521 EDT | AveragePolicySurr           -5.98841
2017-07-31 16:08:18.267966 EDT | AverageQ                     9.71494
2017-07-31 16:08:18.268410 EDT | AverageAbsQ                  9.73144
2017-07-31 16:08:18.268857 EDT | AverageY                     9.71646
2017-07-31 16:08:18.269301 EDT | AverageAbsY                  9.73043
2017-07-31 16:08:18.269743 EDT | AverageAbsQYDiff             0.20043
2017-07-31 16:08:18.270201 EDT | AverageAction                1
2017-07-31 16:08:18.270638 EDT | PolicyRegParamNorm          14.1158
2017-07-31 16:08:18.271076 EDT | QFunRegParamNorm            21.1681
2017-07-31 16:08:18.271512 EDT | -----------------------  ------------
2017-07-31 16:08:18.272181 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #19 | Training started
2017-07-31 16:08:48.491790 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #19 | Training finished
2017-07-31 16:08:48.492693 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #19 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:08:48.492908 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #19 | Collecting samples for evaluation
2017-07-31 16:09:06.899249 EDT | -----------------------  ------------
2017-07-31 16:09:06.900236 EDT | Oracle Interactions      14681
2017-07-31 16:09:06.900499 EDT | Agent Interactions        5319
2017-07-31 16:09:06.900747 EDT | Epoch                       19
2017-07-31 16:09:06.900989 EDT | Iteration                   19
2017-07-31 16:09:06.901233 EDT | AverageReturn               38.7936
2017-07-31 16:09:06.901472 EDT | StdReturn                    0.939674
2017-07-31 16:09:06.901709 EDT | MaxReturn                   40.6013
2017-07-31 16:09:06.901949 EDT | MinReturn                   36.8074
2017-07-31 16:09:06.902219 EDT | AverageEsReturn             27.1083
2017-07-31 16:09:06.902458 EDT | StdEsReturn                  0.507595
2017-07-31 16:09:06.902691 EDT | MaxEsReturn                 27.9505
2017-07-31 16:09:06.902926 EDT | MinEsReturn                 26.1425
2017-07-31 16:09:06.903161 EDT | AverageDiscountedReturn     34.4767
2017-07-31 16:09:06.903404 EDT | AverageQLoss                 0.247751
2017-07-31 16:09:06.903635 EDT | AveragePolicySurr           -6.82773
2017-07-31 16:09:06.903868 EDT | AverageQ                    11.234
2017-07-31 16:09:06.904103 EDT | AverageAbsQ                 11.2496
2017-07-31 16:09:06.904348 EDT | AverageY                    11.2362
2017-07-31 16:09:06.904588 EDT | AverageAbsY                 11.2488
2017-07-31 16:09:06.904822 EDT | AverageAbsQYDiff             0.228385
2017-07-31 16:09:06.905056 EDT | AverageAction                1
2017-07-31 16:09:06.905292 EDT | PolicyRegParamNorm          14.1485
2017-07-31 16:09:06.905527 EDT | QFunRegParamNorm            22.4729
2017-07-31 16:09:06.905766 EDT | -----------------------  ------------
2017-07-31 16:09:06.906225 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #20 | Training started
2017-07-31 16:09:38.482703 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #20 | Training finished
2017-07-31 16:09:38.483499 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #20 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:09:38.483729 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #20 | Collecting samples for evaluation
2017-07-31 16:09:56.840895 EDT | -----------------------  ------------
2017-07-31 16:09:56.842054 EDT | Oracle Interactions      15681
2017-07-31 16:09:56.842397 EDT | Agent Interactions        5319
2017-07-31 16:09:56.842700 EDT | Epoch                       20
2017-07-31 16:09:56.842992 EDT | Iteration                   20
2017-07-31 16:09:56.843281 EDT | AverageReturn               38.7831
2017-07-31 16:09:56.843568 EDT | StdReturn                    0.967927
2017-07-31 16:09:56.843765 EDT | MaxReturn                   40.6323
2017-07-31 16:09:56.843930 EDT | MinReturn                   36.6373
2017-07-31 16:09:56.844098 EDT | AverageEsReturn             27.3908
2017-07-31 16:09:56.844260 EDT | StdEsReturn                  0.549602
2017-07-31 16:09:56.844423 EDT | MaxEsReturn                 28.364
2017-07-31 16:09:56.844586 EDT | MinEsReturn                 26.2431
2017-07-31 16:09:56.844746 EDT | AverageDiscountedReturn     34.4682
2017-07-31 16:09:56.844907 EDT | AverageQLoss                 0.281172
2017-07-31 16:09:56.845068 EDT | AveragePolicySurr           -7.72368
2017-07-31 16:09:56.845230 EDT | AverageQ                    12.8684
2017-07-31 16:09:56.845394 EDT | AverageAbsQ                 12.8828
2017-07-31 16:09:56.845556 EDT | AverageY                    12.87
2017-07-31 16:09:56.845717 EDT | AverageAbsY                 12.8808
2017-07-31 16:09:56.845880 EDT | AverageAbsQYDiff             0.245596
2017-07-31 16:09:56.846040 EDT | AverageAction                1
2017-07-31 16:09:56.846435 EDT | PolicyRegParamNorm          14.3704
2017-07-31 16:09:56.846698 EDT | QFunRegParamNorm            23.9667
2017-07-31 16:09:56.846954 EDT | -----------------------  ------------
2017-07-31 16:09:56.847443 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #21 | Training started
2017-07-31 16:10:26.671809 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #21 | Training finished
2017-07-31 16:10:26.672881 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #21 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:10:26.673498 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #21 | Collecting samples for evaluation
2017-07-31 16:10:45.530919 EDT | -----------------------  ------------
2017-07-31 16:10:45.531831 EDT | Oracle Interactions      16681
2017-07-31 16:10:45.532054 EDT | Agent Interactions        5319
2017-07-31 16:10:45.532237 EDT | Epoch                       21
2017-07-31 16:10:45.532417 EDT | Iteration                   21
2017-07-31 16:10:45.532584 EDT | AverageReturn               38.77
2017-07-31 16:10:45.532767 EDT | StdReturn                    0.890037
2017-07-31 16:10:45.532931 EDT | MaxReturn                   41.7913
2017-07-31 16:10:45.533097 EDT | MinReturn                   36.6399
2017-07-31 16:10:45.533318 EDT | AverageEsReturn             27.2543
2017-07-31 16:10:45.533553 EDT | StdEsReturn                  0.452195
2017-07-31 16:10:45.533731 EDT | MaxEsReturn                 28.2552
2017-07-31 16:10:45.534016 EDT | MinEsReturn                 26.3331
2017-07-31 16:10:45.534260 EDT | AverageDiscountedReturn     34.4589
2017-07-31 16:10:45.534459 EDT | AverageQLoss                 0.309209
2017-07-31 16:10:45.534707 EDT | AveragePolicySurr           -8.56686
2017-07-31 16:10:45.534879 EDT | AverageQ                    14.4963
2017-07-31 16:10:45.535044 EDT | AverageAbsQ                 14.5119
2017-07-31 16:10:45.535300 EDT | AverageY                    14.498
2017-07-31 16:10:45.535481 EDT | AverageAbsY                 14.5087
2017-07-31 16:10:45.535641 EDT | AverageAbsQYDiff             0.249957
2017-07-31 16:10:45.535809 EDT | AverageAction                1
2017-07-31 16:10:45.536005 EDT | PolicyRegParamNorm          14.7954
2017-07-31 16:10:45.536173 EDT | QFunRegParamNorm            25.1581
2017-07-31 16:10:45.536374 EDT | -----------------------  ------------
2017-07-31 16:10:45.536710 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #22 | Training started
2017-07-31 16:11:16.323398 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #22 | Training finished
2017-07-31 16:11:16.324621 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #22 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:11:16.324982 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #22 | Collecting samples for evaluation
2017-07-31 16:11:34.271779 EDT | -----------------------  ------------
2017-07-31 16:11:34.272791 EDT | Oracle Interactions      17681
2017-07-31 16:11:34.273097 EDT | Agent Interactions        5319
2017-07-31 16:11:34.273396 EDT | Epoch                       22
2017-07-31 16:11:34.273688 EDT | Iteration                   22
2017-07-31 16:11:34.273986 EDT | AverageReturn               38.809
2017-07-31 16:11:34.274295 EDT | StdReturn                    0.924348
2017-07-31 16:11:34.274592 EDT | MaxReturn                   40.7856
2017-07-31 16:11:34.274882 EDT | MinReturn                   36.6438
2017-07-31 16:11:34.275180 EDT | AverageEsReturn             27.2616
2017-07-31 16:11:34.275469 EDT | StdEsReturn                  0.443116
2017-07-31 16:11:34.275769 EDT | MaxEsReturn                 27.9948
2017-07-31 16:11:34.276053 EDT | MinEsReturn                 26.4682
2017-07-31 16:11:34.276339 EDT | AverageDiscountedReturn     34.4872
2017-07-31 16:11:34.276626 EDT | AverageQLoss                 0.32602
2017-07-31 16:11:34.276918 EDT | AveragePolicySurr           -9.38589
2017-07-31 16:11:34.277206 EDT | AverageQ                    16.1409
2017-07-31 16:11:34.277491 EDT | AverageAbsQ                 16.157
2017-07-31 16:11:34.277778 EDT | AverageY                    16.1428
2017-07-31 16:11:34.278061 EDT | AverageAbsY                 16.1543
2017-07-31 16:11:34.278363 EDT | AverageAbsQYDiff             0.261292
2017-07-31 16:11:34.278649 EDT | AverageAction                1
2017-07-31 16:11:34.278938 EDT | PolicyRegParamNorm          14.7955
2017-07-31 16:11:34.279228 EDT | QFunRegParamNorm            26.3028
2017-07-31 16:11:34.279519 EDT | -----------------------  ------------
2017-07-31 16:11:34.280038 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #23 | Training started
2017-07-31 16:12:03.821457 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #23 | Training finished
2017-07-31 16:12:03.822273 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #23 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:12:03.822498 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #23 | Collecting samples for evaluation
2017-07-31 16:12:20.837657 EDT | -----------------------  ------------
2017-07-31 16:12:20.838521 EDT | Oracle Interactions      18681
2017-07-31 16:12:20.838862 EDT | Agent Interactions        5319
2017-07-31 16:12:20.839050 EDT | Epoch                       23
2017-07-31 16:12:20.839228 EDT | Iteration                   23
2017-07-31 16:12:20.839436 EDT | AverageReturn               38.807
2017-07-31 16:12:20.839707 EDT | StdReturn                    0.965557
2017-07-31 16:12:20.839910 EDT | MaxReturn                   40.7484
2017-07-31 16:12:20.840246 EDT | MinReturn                   36.6418
2017-07-31 16:12:20.840529 EDT | AverageEsReturn             27.2758
2017-07-31 16:12:20.840692 EDT | StdEsReturn                  0.623137
2017-07-31 16:12:20.840863 EDT | MaxEsReturn                 28.2099
2017-07-31 16:12:20.841038 EDT | MinEsReturn                 25.8085
2017-07-31 16:12:20.841209 EDT | AverageDiscountedReturn     34.4879
2017-07-31 16:12:20.841411 EDT | AverageQLoss                 0.404861
2017-07-31 16:12:20.841579 EDT | AveragePolicySurr          -10.1398
2017-07-31 16:12:20.841744 EDT | AverageQ                    17.6429
2017-07-31 16:12:20.841995 EDT | AverageAbsQ                 17.6616
2017-07-31 16:12:20.842177 EDT | AverageY                    17.6448
2017-07-31 16:12:20.842345 EDT | AverageAbsY                 17.6569
2017-07-31 16:12:20.842507 EDT | AverageAbsQYDiff             0.288861
2017-07-31 16:12:20.842665 EDT | AverageAction                1
2017-07-31 16:12:20.842826 EDT | PolicyRegParamNorm          14.7959
2017-07-31 16:12:20.842977 EDT | QFunRegParamNorm            27.3074
2017-07-31 16:12:20.843141 EDT | -----------------------  ------------
2017-07-31 16:12:20.843477 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #24 | Training started
2017-07-31 16:12:49.914474 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #24 | Training finished
2017-07-31 16:12:49.915448 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #24 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:12:49.915772 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #24 | Collecting samples for evaluation
2017-07-31 16:13:08.043298 EDT | -----------------------  ------------
2017-07-31 16:13:08.044329 EDT | Oracle Interactions      19681
2017-07-31 16:13:08.044637 EDT | Agent Interactions        5319
2017-07-31 16:13:08.044922 EDT | Epoch                       24
2017-07-31 16:13:08.045203 EDT | Iteration                   24
2017-07-31 16:13:08.045483 EDT | AverageReturn               38.8195
2017-07-31 16:13:08.045767 EDT | StdReturn                    0.965651
2017-07-31 16:13:08.046048 EDT | MaxReturn                   40.8172
2017-07-31 16:13:08.046342 EDT | MinReturn                   36.5858
2017-07-31 16:13:08.046629 EDT | AverageEsReturn             27.2896
2017-07-31 16:13:08.046913 EDT | StdEsReturn                  0.705082
2017-07-31 16:13:08.047191 EDT | MaxEsReturn                 28.946
2017-07-31 16:13:08.047472 EDT | MinEsReturn                 26.3352
2017-07-31 16:13:08.047750 EDT | AverageDiscountedReturn     34.4968
2017-07-31 16:13:08.048033 EDT | AverageQLoss                 0.388529
2017-07-31 16:13:08.048315 EDT | AveragePolicySurr          -10.8968
2017-07-31 16:13:08.048595 EDT | AverageQ                    19.2207
2017-07-31 16:13:08.048872 EDT | AverageAbsQ                 19.2396
2017-07-31 16:13:08.049148 EDT | AverageY                    19.2217
2017-07-31 16:13:08.049428 EDT | AverageAbsY                 19.2347
2017-07-31 16:13:08.049704 EDT | AverageAbsQYDiff             0.273719
2017-07-31 16:13:08.049979 EDT | AverageAction                1
2017-07-31 16:13:08.050292 EDT | PolicyRegParamNorm          14.7963
2017-07-31 16:13:08.050581 EDT | QFunRegParamNorm            28.2784
2017-07-31 16:13:08.050873 EDT | -----------------------  ------------
2017-07-31 16:13:08.051366 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #25 | Training started
2017-07-31 16:13:38.084364 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #25 | Training finished
2017-07-31 16:13:38.085512 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #25 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:13:38.086039 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #25 | Collecting samples for evaluation
2017-07-31 16:13:56.804946 EDT | -----------------------  ------------
2017-07-31 16:13:56.805889 EDT | Oracle Interactions      20681
2017-07-31 16:13:56.806115 EDT | Agent Interactions        5319
2017-07-31 16:13:56.806389 EDT | Epoch                       25
2017-07-31 16:13:56.806572 EDT | Iteration                   25
2017-07-31 16:13:56.806747 EDT | AverageReturn               38.8154
2017-07-31 16:13:56.806940 EDT | StdReturn                    0.92469
2017-07-31 16:13:56.807143 EDT | MaxReturn                   40.7447
2017-07-31 16:13:56.807324 EDT | MinReturn                   36.7518
2017-07-31 16:13:56.807557 EDT | AverageEsReturn             26.9976
2017-07-31 16:13:56.807756 EDT | StdEsReturn                  0.47701
2017-07-31 16:13:56.808089 EDT | MaxEsReturn                 27.8696
2017-07-31 16:13:56.808331 EDT | MinEsReturn                 26.2205
2017-07-31 16:13:56.808496 EDT | AverageDiscountedReturn     34.4924
2017-07-31 16:13:56.808652 EDT | AverageQLoss                 0.450817
2017-07-31 16:13:56.808812 EDT | AveragePolicySurr          -11.5659
2017-07-31 16:13:56.808975 EDT | AverageQ                    20.6155
2017-07-31 16:13:56.809132 EDT | AverageAbsQ                 20.6351
2017-07-31 16:13:56.809284 EDT | AverageY                    20.6174
2017-07-31 16:13:56.809437 EDT | AverageAbsY                 20.6303
2017-07-31 16:13:56.809590 EDT | AverageAbsQYDiff             0.299507
2017-07-31 16:13:56.809742 EDT | AverageAction                1
2017-07-31 16:13:56.809900 EDT | PolicyRegParamNorm          14.797
2017-07-31 16:13:56.810053 EDT | QFunRegParamNorm            29.0754
2017-07-31 16:13:56.810245 EDT | -----------------------  ------------
2017-07-31 16:13:56.810670 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #26 | Training started
2017-07-31 16:14:28.238490 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #26 | Training finished
2017-07-31 16:14:28.239456 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #26 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:14:28.239768 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #26 | Collecting samples for evaluation
2017-07-31 16:14:47.516192 EDT | -----------------------  ------------
2017-07-31 16:14:47.517169 EDT | Oracle Interactions      21681
2017-07-31 16:14:47.517480 EDT | Agent Interactions        5319
2017-07-31 16:14:47.517779 EDT | Epoch                       26
2017-07-31 16:14:47.518094 EDT | Iteration                   26
2017-07-31 16:14:47.518980 EDT | AverageReturn               38.78
2017-07-31 16:14:47.519509 EDT | StdReturn                    0.921003
2017-07-31 16:14:47.520025 EDT | MaxReturn                   41.8214
2017-07-31 16:14:47.520518 EDT | MinReturn                   36.7338
2017-07-31 16:14:47.521014 EDT | AverageEsReturn             27.0237
2017-07-31 16:14:47.521498 EDT | StdEsReturn                  0.516074
2017-07-31 16:14:47.521991 EDT | MaxEsReturn                 28.2448
2017-07-31 16:14:47.522501 EDT | MinEsReturn                 25.9295
2017-07-31 16:14:47.522987 EDT | AverageDiscountedReturn     34.4647
2017-07-31 16:14:47.523475 EDT | AverageQLoss                 0.572473
2017-07-31 16:14:47.523959 EDT | AveragePolicySurr          -12.2273
2017-07-31 16:14:47.524442 EDT | AverageQ                    21.951
2017-07-31 16:14:47.524967 EDT | AverageAbsQ                 21.9679
2017-07-31 16:14:47.525498 EDT | AverageY                    21.9517
2017-07-31 16:14:47.528291 EDT | AverageAbsY                 21.9629
2017-07-31 16:14:47.528825 EDT | AverageAbsQYDiff             0.314002
2017-07-31 16:14:47.529283 EDT | AverageAction                1
2017-07-31 16:14:47.529721 EDT | PolicyRegParamNorm          14.7975
2017-07-31 16:14:47.530180 EDT | QFunRegParamNorm            29.902
2017-07-31 16:14:47.530612 EDT | -----------------------  ------------
2017-07-31 16:14:47.531248 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #27 | Training started
2017-07-31 16:15:18.617604 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #27 | Training finished
2017-07-31 16:15:18.618634 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #27 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:15:18.619041 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #27 | Collecting samples for evaluation
2017-07-31 16:15:36.648331 EDT | -----------------------  ------------
2017-07-31 16:15:36.649306 EDT | Oracle Interactions      22681
2017-07-31 16:15:36.649609 EDT | Agent Interactions        5319
2017-07-31 16:15:36.649875 EDT | Epoch                       27
2017-07-31 16:15:36.650158 EDT | Iteration                   27
2017-07-31 16:15:36.650438 EDT | AverageReturn               38.7963
2017-07-31 16:15:36.650728 EDT | StdReturn                    0.963679
2017-07-31 16:15:36.650991 EDT | MaxReturn                   40.6785
2017-07-31 16:15:36.651242 EDT | MinReturn                   36.5697
2017-07-31 16:15:36.651479 EDT | AverageEsReturn             27.2032
2017-07-31 16:15:36.651719 EDT | StdEsReturn                  0.614338
2017-07-31 16:15:36.652010 EDT | MaxEsReturn                 28.4594
2017-07-31 16:15:36.652310 EDT | MinEsReturn                 25.8681
2017-07-31 16:15:36.652555 EDT | AverageDiscountedReturn     34.4776
2017-07-31 16:15:36.652795 EDT | AverageQLoss                 0.560932
2017-07-31 16:15:36.653039 EDT | AveragePolicySurr          -12.7986
2017-07-31 16:15:36.653278 EDT | AverageQ                    23.2317
2017-07-31 16:15:36.653515 EDT | AverageAbsQ                 23.2489
2017-07-31 16:15:36.653747 EDT | AverageY                    23.2337
2017-07-31 16:15:36.653984 EDT | AverageAbsY                 23.2442
2017-07-31 16:15:36.654236 EDT | AverageAbsQYDiff             0.332448
2017-07-31 16:15:36.654488 EDT | AverageAction                1
2017-07-31 16:15:36.654721 EDT | PolicyRegParamNorm          14.798
2017-07-31 16:15:36.654953 EDT | QFunRegParamNorm            30.8446
2017-07-31 16:15:36.655185 EDT | -----------------------  ------------
2017-07-31 16:15:36.655647 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #28 | Training started
2017-07-31 16:16:07.728514 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #28 | Training finished
2017-07-31 16:16:07.729619 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #28 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:16:07.729988 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #28 | Collecting samples for evaluation
2017-07-31 16:16:26.229468 EDT | -----------------------  ------------
2017-07-31 16:16:26.230314 EDT | Oracle Interactions      23681
2017-07-31 16:16:26.230686 EDT | Agent Interactions        5319
2017-07-31 16:16:26.230997 EDT | Epoch                       28
2017-07-31 16:16:26.231282 EDT | Iteration                   28
2017-07-31 16:16:26.231471 EDT | AverageReturn               38.8136
2017-07-31 16:16:26.231638 EDT | StdReturn                    0.900636
2017-07-31 16:16:26.231925 EDT | MaxReturn                   40.8938
2017-07-31 16:16:26.232113 EDT | MinReturn                   36.6475
2017-07-31 16:16:26.232425 EDT | AverageEsReturn             27.2474
2017-07-31 16:16:26.232636 EDT | StdEsReturn                  0.63307
2017-07-31 16:16:26.232840 EDT | MaxEsReturn                 28.6164
2017-07-31 16:16:26.233166 EDT | MinEsReturn                 26.4138
2017-07-31 16:16:26.233378 EDT | AverageDiscountedReturn     34.491
2017-07-31 16:16:26.233697 EDT | AverageQLoss                 0.54793
2017-07-31 16:16:26.233861 EDT | AveragePolicySurr          -13.2075
2017-07-31 16:16:26.234087 EDT | AverageQ                    24.2353
2017-07-31 16:16:26.234310 EDT | AverageAbsQ                 24.2519
2017-07-31 16:16:26.234469 EDT | AverageY                    24.2364
2017-07-31 16:16:26.234764 EDT | AverageAbsY                 24.2465
2017-07-31 16:16:26.235102 EDT | AverageAbsQYDiff             0.320799
2017-07-31 16:16:26.235433 EDT | AverageAction                1
2017-07-31 16:16:26.235611 EDT | PolicyRegParamNorm          14.7985
2017-07-31 16:16:26.235805 EDT | QFunRegParamNorm            31.5441
2017-07-31 16:16:26.236024 EDT | -----------------------  ------------
2017-07-31 16:16:26.236475 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #29 | Training started
2017-07-31 16:16:56.654454 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #29 | Training finished
2017-07-31 16:16:56.655529 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #29 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:16:56.655899 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_1] epoch #29 | Collecting samples for evaluation
2017-07-31 16:17:14.976589 EDT | -----------------------  ------------
2017-07-31 16:17:14.980622 EDT | Oracle Interactions      24681
2017-07-31 16:17:14.980988 EDT | Agent Interactions        5319
2017-07-31 16:17:14.981292 EDT | Epoch                       29
2017-07-31 16:17:14.981592 EDT | Iteration                   29
2017-07-31 16:17:14.981888 EDT | AverageReturn               38.767
2017-07-31 16:17:14.982185 EDT | StdReturn                    0.949483
2017-07-31 16:17:14.982508 EDT | MaxReturn                   40.7491
2017-07-31 16:17:14.982814 EDT | MinReturn                   36.7012
2017-07-31 16:17:14.983103 EDT | AverageEsReturn             27.3662
2017-07-31 16:17:14.983418 EDT | StdEsReturn                  0.72481
2017-07-31 16:17:14.983747 EDT | MaxEsReturn                 28.5363
2017-07-31 16:17:14.984044 EDT | MinEsReturn                 26.0205
2017-07-31 16:17:14.984344 EDT | AverageDiscountedReturn     34.4559
2017-07-31 16:17:14.984657 EDT | AverageQLoss                 0.578082
2017-07-31 16:17:14.984972 EDT | AveragePolicySurr          -13.5763
2017-07-31 16:17:14.985287 EDT | AverageQ                    25.114
2017-07-31 16:17:14.985630 EDT | AverageAbsQ                 25.1323
2017-07-31 16:17:14.985967 EDT | AverageY                    25.1145
2017-07-31 16:17:14.986322 EDT | AverageAbsY                 25.1257
2017-07-31 16:17:14.986637 EDT | AverageAbsQYDiff             0.321156
2017-07-31 16:17:14.986949 EDT | AverageAction                1
2017-07-31 16:17:14.987248 EDT | PolicyRegParamNorm          14.7991
2017-07-31 16:17:14.987564 EDT | QFunRegParamNorm            32.2864
2017-07-31 16:17:14.987882 EDT | -----------------------  ------------
