2017-07-31 16:28:04.749739 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] observation space: Box(11,)
2017-07-31 16:28:04.750148 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] action space: Box(3,)
2017-07-31 16:28:05.232268 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] Populating workers...
2017-07-31 16:28:05.232710 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] Populated
2017-07-31 16:28:08.054651 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #0 | Training started
2017-07-31 16:28:10.520335 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #0 | Training finished
2017-07-31 16:28:10.520711 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #0 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:10.521218 EDT | -------------------  ----
2017-07-31 16:28:10.521416 EDT | Oracle Interactions  1000
2017-07-31 16:28:10.521597 EDT | Agent Interactions      0
2017-07-31 16:28:10.521770 EDT | -------------------  ----
2017-07-31 16:28:10.522067 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #1 | Training started
2017-07-31 16:28:13.416542 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #1 | Training finished
2017-07-31 16:28:13.417150 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #1 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:13.417776 EDT | -------------------  ----
2017-07-31 16:28:13.418085 EDT | Oracle Interactions  2000
2017-07-31 16:28:13.418387 EDT | Agent Interactions      0
2017-07-31 16:28:13.418678 EDT | -------------------  ----
2017-07-31 16:28:13.419041 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #2 | Training started
2017-07-31 16:28:16.098249 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #2 | Training finished
2017-07-31 16:28:16.098618 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #2 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:16.099181 EDT | -------------------  ----
2017-07-31 16:28:16.099460 EDT | Oracle Interactions  3000
2017-07-31 16:28:16.099729 EDT | Agent Interactions      0
2017-07-31 16:28:16.099993 EDT | -------------------  ----
2017-07-31 16:28:16.100332 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #3 | Training started
2017-07-31 16:28:18.516416 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #3 | Training finished
2017-07-31 16:28:18.516731 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #3 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:18.517226 EDT | -------------------  ----
2017-07-31 16:28:18.517606 EDT | Oracle Interactions  4000
2017-07-31 16:28:18.517813 EDT | Agent Interactions      0
2017-07-31 16:28:18.517990 EDT | -------------------  ----
2017-07-31 16:28:18.518374 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #4 | Training started
2017-07-31 16:28:21.197130 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #4 | Training finished
2017-07-31 16:28:21.197998 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #4 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:21.198646 EDT | -------------------  ----
2017-07-31 16:28:21.198956 EDT | Oracle Interactions  5000
2017-07-31 16:28:21.199256 EDT | Agent Interactions      0
2017-07-31 16:28:21.199543 EDT | -------------------  ----
2017-07-31 16:28:21.199915 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #5 | Training started
2017-07-31 16:28:23.481829 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #5 | Training finished
2017-07-31 16:28:23.482324 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #5 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:23.482953 EDT | -------------------  ----
2017-07-31 16:28:23.483220 EDT | Oracle Interactions  6000
2017-07-31 16:28:23.483465 EDT | Agent Interactions      0
2017-07-31 16:28:23.483701 EDT | -------------------  ----
2017-07-31 16:28:23.484044 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #6 | Training started
2017-07-31 16:28:26.246633 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #6 | Training finished
2017-07-31 16:28:26.246955 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #6 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:26.247407 EDT | -------------------  ----
2017-07-31 16:28:26.247789 EDT | Oracle Interactions  7000
2017-07-31 16:28:26.248089 EDT | Agent Interactions      0
2017-07-31 16:28:26.248260 EDT | -------------------  ----
2017-07-31 16:28:26.248487 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #7 | Training started
2017-07-31 16:28:28.795795 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #7 | Training finished
2017-07-31 16:28:28.796683 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #7 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:28.797433 EDT | -------------------  ----
2017-07-31 16:28:28.797781 EDT | Oracle Interactions  8000
2017-07-31 16:28:28.798123 EDT | Agent Interactions      0
2017-07-31 16:28:28.798428 EDT | -------------------  ----
2017-07-31 16:28:28.798851 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #8 | Training started
2017-07-31 16:28:31.308330 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #8 | Training finished
2017-07-31 16:28:31.308897 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #8 | Trained qf 0 steps, policy 0 steps
2017-07-31 16:28:31.309681 EDT | -------------------  ----
2017-07-31 16:28:31.310094 EDT | Oracle Interactions  9000
2017-07-31 16:28:31.310518 EDT | Agent Interactions      0
2017-07-31 16:28:31.310915 EDT | -------------------  ----
2017-07-31 16:28:31.311386 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #9 | Training started
2017-07-31 16:28:33.914874 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #9 | Training finished
2017-07-31 16:28:33.915231 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #9 | Trained qf 1 steps, policy 1 steps
2017-07-31 16:28:33.915554 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #9 | Collecting samples for evaluation
2017-07-31 16:28:52.208506 EDT | -----------------------  -------------
2017-07-31 16:28:52.209665 EDT | Oracle Interactions      10000
2017-07-31 16:28:52.210020 EDT | Agent Interactions           0
2017-07-31 16:28:52.210374 EDT | Epoch                        9
2017-07-31 16:28:52.210699 EDT | Iteration                    9
2017-07-31 16:28:52.211008 EDT | AverageReturn                5.32653
2017-07-31 16:28:52.211318 EDT | StdReturn                    0.31778
2017-07-31 16:28:52.211632 EDT | MaxReturn                    5.94737
2017-07-31 16:28:52.211925 EDT | MinReturn                    4.64938
2017-07-31 16:28:52.212217 EDT | AverageEsReturn             17.8639
2017-07-31 16:28:52.212505 EDT | StdEsReturn                  0.831828
2017-07-31 16:28:52.212798 EDT | MaxEsReturn                 19.9078
2017-07-31 16:28:52.213082 EDT | MinEsReturn                 16.9846
2017-07-31 16:28:52.213364 EDT | AverageDiscountedReturn      5.9604
2017-07-31 16:28:52.213650 EDT | AverageQLoss                 1.01073
2017-07-31 16:28:52.213931 EDT | AveragePolicySurr           -0.0183498
2017-07-31 16:28:52.214234 EDT | AverageQ                    -0.159554
2017-07-31 16:28:52.214535 EDT | AverageAbsQ                  0.159554
2017-07-31 16:28:52.214838 EDT | AverageY                     0.845242
2017-07-31 16:28:52.215136 EDT | AverageAbsY                  0.845242
2017-07-31 16:28:52.215429 EDT | AverageAbsQYDiff             1.0048
2017-07-31 16:28:52.215719 EDT | AverageAction                0.0918372
2017-07-31 16:28:52.216007 EDT | PolicyRegParamNorm          13.2797
2017-07-31 16:28:52.216288 EDT | QFunRegParamNorm            11.1181
2017-07-31 16:28:52.216577 EDT | -----------------------  -------------
2017-07-31 16:28:52.217242 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #10 | Training started
2017-07-31 16:29:22.808317 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #10 | Training finished
2017-07-31 16:29:22.809241 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #10 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:29:22.809500 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #10 | Collecting samples for evaluation
2017-07-31 16:29:42.542996 EDT | -----------------------  -------------
2017-07-31 16:29:42.543964 EDT | Oracle Interactions      10051
2017-07-31 16:29:42.544260 EDT | Agent Interactions         949
2017-07-31 16:29:42.544808 EDT | Epoch                       10
2017-07-31 16:29:42.545287 EDT | Iteration                   10
2017-07-31 16:29:42.545747 EDT | AverageReturn                5.8304
2017-07-31 16:29:42.546243 EDT | StdReturn                    0.0589454
2017-07-31 16:29:42.546702 EDT | MaxReturn                    6.00723
2017-07-31 16:29:42.547163 EDT | MinReturn                    5.69709
2017-07-31 16:29:42.547615 EDT | AverageEsReturn             14.7531
2017-07-31 16:29:42.548066 EDT | StdEsReturn                 17.0405
2017-07-31 16:29:42.548515 EDT | MaxEsReturn                 78.2738
2017-07-31 16:29:42.548965 EDT | MinEsReturn                  4.13106
2017-07-31 16:29:42.549433 EDT | AverageDiscountedReturn      5.6715
2017-07-31 16:29:42.549876 EDT | AverageQLoss                 0.0144943
2017-07-31 16:29:42.550332 EDT | AveragePolicySurr           -1.26884
2017-07-31 16:29:42.550775 EDT | AverageQ                     1.32492
2017-07-31 16:29:42.551215 EDT | AverageAbsQ                  1.32507
2017-07-31 16:29:42.551665 EDT | AverageY                     1.33177
2017-07-31 16:29:42.552111 EDT | AverageAbsY                  1.33185
2017-07-31 16:29:42.552552 EDT | AverageAbsQYDiff             0.0559263
2017-07-31 16:29:42.553004 EDT | AverageAction                0.958858
2017-07-31 16:29:42.553450 EDT | PolicyRegParamNorm          15.0388
2017-07-31 16:29:42.553887 EDT | QFunRegParamNorm            11.9774
2017-07-31 16:29:42.554338 EDT | -----------------------  -------------
2017-07-31 16:29:42.555002 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #11 | Training started
2017-07-31 16:30:12.859846 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #11 | Training finished
2017-07-31 16:30:12.860787 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #11 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:30:12.861021 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #11 | Collecting samples for evaluation
2017-07-31 16:30:31.786044 EDT | -----------------------  -------------
2017-07-31 16:30:31.786965 EDT | Oracle Interactions      10051
2017-07-31 16:30:31.787254 EDT | Agent Interactions        1949
2017-07-31 16:30:31.787513 EDT | Epoch                       11
2017-07-31 16:30:31.787771 EDT | Iteration                   11
2017-07-31 16:30:31.788026 EDT | AverageReturn                5.84102
2017-07-31 16:30:31.788276 EDT | StdReturn                    0.0586665
2017-07-31 16:30:31.788524 EDT | MaxReturn                    6.01038
2017-07-31 16:30:31.788769 EDT | MinReturn                    5.69525
2017-07-31 16:30:31.789015 EDT | AverageEsReturn              6.59829
2017-07-31 16:30:31.789262 EDT | StdEsReturn                  1.43064
2017-07-31 16:30:31.789510 EDT | MaxEsReturn                 13.8548
2017-07-31 16:30:31.789763 EDT | MinEsReturn                  4.24423
2017-07-31 16:30:31.790013 EDT | AverageDiscountedReturn      5.68187
2017-07-31 16:30:31.790313 EDT | AverageQLoss                 0.0219633
2017-07-31 16:30:31.790567 EDT | AveragePolicySurr           -1.61238
2017-07-31 16:30:31.790818 EDT | AverageQ                     2.16192
2017-07-31 16:30:31.791064 EDT | AverageAbsQ                  2.16198
2017-07-31 16:30:31.791344 EDT | AverageY                     2.16304
2017-07-31 16:30:31.791593 EDT | AverageAbsY                  2.16318
2017-07-31 16:30:31.791844 EDT | AverageAbsQYDiff             0.0705572
2017-07-31 16:30:31.792090 EDT | AverageAction                0.976215
2017-07-31 16:30:31.792367 EDT | PolicyRegParamNorm          15.6427
2017-07-31 16:30:31.792615 EDT | QFunRegParamNorm            12.7114
2017-07-31 16:30:31.792866 EDT | -----------------------  -------------
2017-07-31 16:30:31.793300 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #12 | Training started
2017-07-31 16:31:03.018170 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #12 | Training finished
2017-07-31 16:31:03.021094 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #12 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:31:03.021460 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #12 | Collecting samples for evaluation
2017-07-31 16:31:21.541574 EDT | -----------------------  -------------
2017-07-31 16:31:21.542474 EDT | Oracle Interactions      10946
2017-07-31 16:31:21.542746 EDT | Agent Interactions        2054
2017-07-31 16:31:21.542995 EDT | Epoch                       12
2017-07-31 16:31:21.543239 EDT | Iteration                   12
2017-07-31 16:31:21.543481 EDT | AverageReturn               38.7449
2017-07-31 16:31:21.543722 EDT | StdReturn                    0.932304
2017-07-31 16:31:21.543962 EDT | MaxReturn                   40.6411
2017-07-31 16:31:21.544202 EDT | MinReturn                   36.5799
2017-07-31 16:31:21.544442 EDT | AverageEsReturn             16.7043
2017-07-31 16:31:21.544680 EDT | StdEsReturn                  6.34012
2017-07-31 16:31:21.544918 EDT | MaxEsReturn                 52.4331
2017-07-31 16:31:21.545157 EDT | MinEsReturn                  5.771
2017-07-31 16:31:21.545396 EDT | AverageDiscountedReturn     34.4365
2017-07-31 16:31:21.545639 EDT | AverageQLoss                 0.0283492
2017-07-31 16:31:21.545877 EDT | AveragePolicySurr           -2.08853
2017-07-31 16:31:21.546115 EDT | AverageQ                     2.91776
2017-07-31 16:31:21.546366 EDT | AverageAbsQ                  2.91785
2017-07-31 16:31:21.546602 EDT | AverageY                     2.91891
2017-07-31 16:31:21.546841 EDT | AverageAbsY                  2.91911
2017-07-31 16:31:21.547074 EDT | AverageAbsQYDiff             0.0761463
2017-07-31 16:31:21.547310 EDT | AverageAction                0.962289
2017-07-31 16:31:21.547548 EDT | PolicyRegParamNorm          16.6702
2017-07-31 16:31:21.547785 EDT | QFunRegParamNorm            13.4997
2017-07-31 16:31:21.548021 EDT | -----------------------  -------------
2017-07-31 16:31:21.548425 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #13 | Training started
2017-07-31 16:31:53.023485 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #13 | Training finished
2017-07-31 16:31:53.026184 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #13 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:31:53.026518 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #13 | Collecting samples for evaluation
2017-07-31 16:32:11.042441 EDT | -----------------------  -------------
2017-07-31 16:32:11.045454 EDT | Oracle Interactions      11946
2017-07-31 16:32:11.045750 EDT | Agent Interactions        2054
2017-07-31 16:32:11.045996 EDT | Epoch                       13
2017-07-31 16:32:11.046287 EDT | Iteration                   13
2017-07-31 16:32:11.046535 EDT | AverageReturn              125.754
2017-07-31 16:32:11.046778 EDT | StdReturn                    0.599817
2017-07-31 16:32:11.047021 EDT | MaxReturn                  127.371
2017-07-31 16:32:11.047263 EDT | MinReturn                  124.211
2017-07-31 16:32:11.047499 EDT | AverageEsReturn             17.605
2017-07-31 16:32:11.047736 EDT | StdEsReturn                  1.89864
2017-07-31 16:32:11.047968 EDT | MaxEsReturn                 20.0453
2017-07-31 16:32:11.048217 EDT | MinEsReturn                  5.07982
2017-07-31 16:32:11.048454 EDT | AverageDiscountedReturn     90.119
2017-07-31 16:32:11.048687 EDT | AverageQLoss                 0.0344025
2017-07-31 16:32:11.048921 EDT | AveragePolicySurr           -2.5919
2017-07-31 16:32:11.049160 EDT | AverageQ                     3.79649
2017-07-31 16:32:11.049393 EDT | AverageAbsQ                  3.79654
2017-07-31 16:32:11.049626 EDT | AverageY                     3.7972
2017-07-31 16:32:11.049865 EDT | AverageAbsY                  3.79743
2017-07-31 16:32:11.050096 EDT | AverageAbsQYDiff             0.0792235
2017-07-31 16:32:11.050348 EDT | AverageAction                0.811893
2017-07-31 16:32:11.050584 EDT | PolicyRegParamNorm          17.5971
2017-07-31 16:32:11.050823 EDT | QFunRegParamNorm            14.451
2017-07-31 16:32:11.051055 EDT | -----------------------  -------------
2017-07-31 16:32:11.051478 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #14 | Training started
2017-07-31 16:32:42.142333 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #14 | Training finished
2017-07-31 16:32:42.147428 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #14 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:32:42.147790 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #14 | Collecting samples for evaluation
2017-07-31 16:33:00.708975 EDT | -----------------------  -------------
2017-07-31 16:33:00.709873 EDT | Oracle Interactions      12946
2017-07-31 16:33:00.710159 EDT | Agent Interactions        2054
2017-07-31 16:33:00.710405 EDT | Epoch                       14
2017-07-31 16:33:00.710638 EDT | Iteration                   14
2017-07-31 16:33:00.710874 EDT | AverageReturn               44.9777
2017-07-31 16:33:00.711103 EDT | StdReturn                    0.603538
2017-07-31 16:33:00.711330 EDT | MaxReturn                   46.6738
2017-07-31 16:33:00.711556 EDT | MinReturn                   43.8654
2017-07-31 16:33:00.711781 EDT | AverageEsReturn             17.9563
2017-07-31 16:33:00.712006 EDT | StdEsReturn                  0.818901
2017-07-31 16:33:00.712231 EDT | MaxEsReturn                 20.0259
2017-07-31 16:33:00.712456 EDT | MinEsReturn                 16.9965
2017-07-31 16:33:00.712681 EDT | AverageDiscountedReturn     39.3653
2017-07-31 16:33:00.712910 EDT | AverageQLoss                 0.0485787
2017-07-31 16:33:00.713135 EDT | AveragePolicySurr           -3.14134
2017-07-31 16:33:00.713361 EDT | AverageQ                     4.7427
2017-07-31 16:33:00.713585 EDT | AverageAbsQ                  4.74273
2017-07-31 16:33:00.713809 EDT | AverageY                     4.74363
2017-07-31 16:33:00.714034 EDT | AverageAbsY                  4.74377
2017-07-31 16:33:00.715710 EDT | AverageAbsQYDiff             0.0903041
2017-07-31 16:33:00.716159 EDT | AverageAction                0.961799
2017-07-31 16:33:00.717230 EDT | PolicyRegParamNorm          18.1526
2017-07-31 16:33:00.717713 EDT | QFunRegParamNorm            15.4965
2017-07-31 16:33:00.718162 EDT | -----------------------  -------------
2017-07-31 16:33:00.718767 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #15 | Training started
2017-07-31 16:33:32.990127 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #15 | Training finished
2017-07-31 16:33:32.991038 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #15 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:33:32.991341 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #15 | Collecting samples for evaluation
2017-07-31 16:33:50.543813 EDT | -----------------------  -------------
2017-07-31 16:33:50.545300 EDT | Oracle Interactions      13946
2017-07-31 16:33:50.545712 EDT | Agent Interactions        2054
2017-07-31 16:33:50.546101 EDT | Epoch                       15
2017-07-31 16:33:50.546503 EDT | Iteration                   15
2017-07-31 16:33:50.546891 EDT | AverageReturn               59.076
2017-07-31 16:33:50.547277 EDT | StdReturn                    1.51132
2017-07-31 16:33:50.547645 EDT | MaxReturn                   63.0521
2017-07-31 16:33:50.548035 EDT | MinReturn                   55.0676
2017-07-31 16:33:50.548422 EDT | AverageEsReturn             17.9215
2017-07-31 16:33:50.548810 EDT | StdEsReturn                  0.944646
2017-07-31 16:33:50.549175 EDT | MaxEsReturn                 19.8017
2017-07-31 16:33:50.549560 EDT | MinEsReturn                 14.8979
2017-07-31 16:33:50.549949 EDT | AverageDiscountedReturn     49.8676
2017-07-31 16:33:50.551338 EDT | AverageQLoss                 0.0573926
2017-07-31 16:33:50.551954 EDT | AveragePolicySurr           -3.73058
2017-07-31 16:33:50.552580 EDT | AverageQ                     5.80132
2017-07-31 16:33:50.553210 EDT | AverageAbsQ                  5.80135
2017-07-31 16:33:50.553831 EDT | AverageY                     5.80187
2017-07-31 16:33:50.554450 EDT | AverageAbsY                  5.80201
2017-07-31 16:33:50.555073 EDT | AverageAbsQYDiff             0.0955529
2017-07-31 16:33:50.555692 EDT | AverageAction                0.901996
2017-07-31 16:33:50.556319 EDT | PolicyRegParamNorm          18.6338
2017-07-31 16:33:50.556959 EDT | QFunRegParamNorm            16.5249
2017-07-31 16:33:50.557556 EDT | -----------------------  -------------
2017-07-31 16:33:50.558512 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #16 | Training started
2017-07-31 16:34:20.867628 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #16 | Training finished
2017-07-31 16:34:20.868719 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #16 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:34:20.869029 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #16 | Collecting samples for evaluation
2017-07-31 16:34:39.376988 EDT | -----------------------  -------------
2017-07-31 16:34:39.377920 EDT | Oracle Interactions      14946
2017-07-31 16:34:39.378217 EDT | Agent Interactions        2054
2017-07-31 16:34:39.378468 EDT | Epoch                       16
2017-07-31 16:34:39.378712 EDT | Iteration                   16
2017-07-31 16:34:39.378950 EDT | AverageReturn               68.9105
2017-07-31 16:34:39.379188 EDT | StdReturn                    4.20598
2017-07-31 16:34:39.379423 EDT | MaxReturn                   72.6805
2017-07-31 16:34:39.379658 EDT | MinReturn                   56.5634
2017-07-31 16:34:39.379892 EDT | AverageEsReturn             17.7111
2017-07-31 16:34:39.380127 EDT | StdEsReturn                  0.953636
2017-07-31 16:34:39.380361 EDT | MaxEsReturn                 19.8672
2017-07-31 16:34:39.380595 EDT | MinEsReturn                 14.1232
2017-07-31 16:34:39.380828 EDT | AverageDiscountedReturn     56.6565
2017-07-31 16:34:39.381060 EDT | AverageQLoss                 0.0658855
2017-07-31 16:34:39.381294 EDT | AveragePolicySurr           -4.34344
2017-07-31 16:34:39.381526 EDT | AverageQ                     6.89203
2017-07-31 16:34:39.381759 EDT | AverageAbsQ                  6.89216
2017-07-31 16:34:39.381992 EDT | AverageY                     6.89262
2017-07-31 16:34:39.382467 EDT | AverageAbsY                  6.89265
2017-07-31 16:34:39.382718 EDT | AverageAbsQYDiff             0.104142
2017-07-31 16:34:39.382963 EDT | AverageAction                0.895325
2017-07-31 16:34:39.383203 EDT | PolicyRegParamNorm          19.2067
2017-07-31 16:34:39.383440 EDT | QFunRegParamNorm            17.3688
2017-07-31 16:34:39.383676 EDT | -----------------------  -------------
2017-07-31 16:34:39.384085 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #17 | Training started
2017-07-31 16:35:10.058758 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #17 | Training finished
2017-07-31 16:35:10.059732 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #17 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:35:10.060059 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #17 | Collecting samples for evaluation
2017-07-31 16:35:28.960296 EDT | -----------------------  -------------
2017-07-31 16:35:28.961250 EDT | Oracle Interactions      15946
2017-07-31 16:35:28.961525 EDT | Agent Interactions        2054
2017-07-31 16:35:28.961786 EDT | Epoch                       17
2017-07-31 16:35:28.962047 EDT | Iteration                   17
2017-07-31 16:35:28.962308 EDT | AverageReturn               52.3248
2017-07-31 16:35:28.962560 EDT | StdReturn                    1.06785
2017-07-31 16:35:28.962810 EDT | MaxReturn                   68.7948
2017-07-31 16:35:28.963062 EDT | MinReturn                   51.2303
2017-07-31 16:35:28.963309 EDT | AverageEsReturn             17.9443
2017-07-31 16:35:28.963558 EDT | StdEsReturn                  0.743836
2017-07-31 16:35:28.963805 EDT | MaxEsReturn                 19.6952
2017-07-31 16:35:28.964054 EDT | MinEsReturn                 16.9937
2017-07-31 16:35:28.964296 EDT | AverageDiscountedReturn     45.0181
2017-07-31 16:35:28.964540 EDT | AverageQLoss                 0.0858992
2017-07-31 16:35:28.964781 EDT | AveragePolicySurr           -5.01586
2017-07-31 16:35:28.965021 EDT | AverageQ                     8.03899
2017-07-31 16:35:28.965263 EDT | AverageAbsQ                  8.0391
2017-07-31 16:35:28.965506 EDT | AverageY                     8.04021
2017-07-31 16:35:28.965755 EDT | AverageAbsY                  8.04027
2017-07-31 16:35:28.966001 EDT | AverageAbsQYDiff             0.123236
2017-07-31 16:35:28.966252 EDT | AverageAction                0.907544
2017-07-31 16:35:28.966489 EDT | PolicyRegParamNorm          19.6813
2017-07-31 16:35:28.966724 EDT | QFunRegParamNorm            18.1938
2017-07-31 16:35:28.966962 EDT | -----------------------  -------------
2017-07-31 16:35:28.967404 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #18 | Training started
2017-07-31 16:35:59.825076 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #18 | Training finished
2017-07-31 16:35:59.826284 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #18 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:35:59.826862 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #18 | Collecting samples for evaluation
2017-07-31 16:36:17.611505 EDT | -----------------------  ------------
2017-07-31 16:36:17.612596 EDT | Oracle Interactions      16946
2017-07-31 16:36:17.612921 EDT | Agent Interactions        2054
2017-07-31 16:36:17.613216 EDT | Epoch                       18
2017-07-31 16:36:17.613503 EDT | Iteration                   18
2017-07-31 16:36:17.613790 EDT | AverageReturn               37.3484
2017-07-31 16:36:17.614074 EDT | StdReturn                    8.52639
2017-07-31 16:36:17.614372 EDT | MaxReturn                   72.8539
2017-07-31 16:36:17.614657 EDT | MinReturn                   33.0529
2017-07-31 16:36:17.614939 EDT | AverageEsReturn             17.991
2017-07-31 16:36:17.615221 EDT | StdEsReturn                  0.987324
2017-07-31 16:36:17.615503 EDT | MaxEsReturn                 19.9365
2017-07-31 16:36:17.615784 EDT | MinEsReturn                 14.1112
2017-07-31 16:36:17.616064 EDT | AverageDiscountedReturn     32.8483
2017-07-31 16:36:17.616345 EDT | AverageQLoss                 0.100617
2017-07-31 16:36:17.616625 EDT | AveragePolicySurr           -5.77903
2017-07-31 16:36:17.616906 EDT | AverageQ                     9.33863
2017-07-31 16:36:17.617185 EDT | AverageAbsQ                  9.33877
2017-07-31 16:36:17.617469 EDT | AverageY                     9.33945
2017-07-31 16:36:17.617750 EDT | AverageAbsY                  9.33954
2017-07-31 16:36:17.618032 EDT | AverageAbsQYDiff             0.126801
2017-07-31 16:36:17.618322 EDT | AverageAction                0.968534
2017-07-31 16:36:17.618602 EDT | PolicyRegParamNorm          20.2439
2017-07-31 16:36:17.618883 EDT | QFunRegParamNorm            19.0997
2017-07-31 16:36:17.619163 EDT | -----------------------  ------------
2017-07-31 16:36:17.619658 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #19 | Training started
2017-07-31 16:36:48.732530 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #19 | Training finished
2017-07-31 16:36:48.733769 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #19 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:36:48.734026 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #19 | Collecting samples for evaluation
2017-07-31 16:37:06.057052 EDT | -----------------------  ------------
2017-07-31 16:37:06.059810 EDT | Oracle Interactions      17946
2017-07-31 16:37:06.060018 EDT | Agent Interactions        2054
2017-07-31 16:37:06.060189 EDT | Epoch                       19
2017-07-31 16:37:06.060421 EDT | Iteration                   19
2017-07-31 16:37:06.060601 EDT | AverageReturn               35.2749
2017-07-31 16:37:06.060837 EDT | StdReturn                    4.43122
2017-07-31 16:37:06.061087 EDT | MaxReturn                   56.9041
2017-07-31 16:37:06.061253 EDT | MinReturn                   32.844
2017-07-31 16:37:06.061418 EDT | AverageEsReturn             17.6904
2017-07-31 16:37:06.061668 EDT | StdEsReturn                  1.42502
2017-07-31 16:37:06.061827 EDT | MaxEsReturn                 19.7996
2017-07-31 16:37:06.061979 EDT | MinEsReturn                  8.82025
2017-07-31 16:37:06.062151 EDT | AverageDiscountedReturn     31.32
2017-07-31 16:37:06.062317 EDT | AverageQLoss                 0.130396
2017-07-31 16:37:06.062474 EDT | AveragePolicySurr           -6.60832
2017-07-31 16:37:06.062632 EDT | AverageQ                    10.7329
2017-07-31 16:37:06.062903 EDT | AverageAbsQ                 10.7332
2017-07-31 16:37:06.063129 EDT | AverageY                    10.7341
2017-07-31 16:37:06.063296 EDT | AverageAbsY                 10.7343
2017-07-31 16:37:06.063452 EDT | AverageAbsQYDiff             0.141093
2017-07-31 16:37:06.063651 EDT | AverageAction                0.969656
2017-07-31 16:37:06.063825 EDT | PolicyRegParamNorm          20.5527
2017-07-31 16:37:06.064057 EDT | QFunRegParamNorm            19.9486
2017-07-31 16:37:06.064217 EDT | -----------------------  ------------
2017-07-31 16:37:06.064505 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #20 | Training started
2017-07-31 16:37:36.114697 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #20 | Training finished
2017-07-31 16:37:36.115708 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #20 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:37:36.116043 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #20 | Collecting samples for evaluation
2017-07-31 16:37:53.986586 EDT | -----------------------  ------------
2017-07-31 16:37:53.987545 EDT | Oracle Interactions      18946
2017-07-31 16:37:53.987897 EDT | Agent Interactions        2054
2017-07-31 16:37:53.988255 EDT | Epoch                       20
2017-07-31 16:37:53.988550 EDT | Iteration                   20
2017-07-31 16:37:53.988714 EDT | AverageReturn               75.5619
2017-07-31 16:37:53.988882 EDT | StdReturn                   14.306
2017-07-31 16:37:53.989044 EDT | MaxReturn                  109.647
2017-07-31 16:37:53.989247 EDT | MinReturn                   61.4711
2017-07-31 16:37:53.989441 EDT | AverageEsReturn             17.6432
2017-07-31 16:37:53.989596 EDT | StdEsReturn                  0.802785
2017-07-31 16:37:53.989754 EDT | MaxEsReturn                 19.7582
2017-07-31 16:37:53.989911 EDT | MinEsReturn                 15.6989
2017-07-31 16:37:53.990081 EDT | AverageDiscountedReturn     61.1932
2017-07-31 16:37:53.990270 EDT | AverageQLoss                 0.130143
2017-07-31 16:37:53.990425 EDT | AveragePolicySurr           -7.40029
2017-07-31 16:37:53.990625 EDT | AverageQ                    12.2232
2017-07-31 16:37:53.990957 EDT | AverageAbsQ                 12.2235
2017-07-31 16:37:53.991273 EDT | AverageY                    12.2238
2017-07-31 16:37:53.991432 EDT | AverageAbsY                 12.2239
2017-07-31 16:37:53.991605 EDT | AverageAbsQYDiff             0.138348
2017-07-31 16:37:53.991771 EDT | AverageAction                0.934085
2017-07-31 16:37:53.991921 EDT | PolicyRegParamNorm          21.0344
2017-07-31 16:37:53.992158 EDT | QFunRegParamNorm            20.696
2017-07-31 16:37:53.992483 EDT | -----------------------  ------------
2017-07-31 16:37:53.992953 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #21 | Training started
2017-07-31 16:38:25.352462 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #21 | Training finished
2017-07-31 16:38:25.353231 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #21 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:38:25.353484 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #21 | Collecting samples for evaluation
2017-07-31 16:38:42.513709 EDT | -----------------------  ------------
2017-07-31 16:38:42.514629 EDT | Oracle Interactions      19946
2017-07-31 16:38:42.514891 EDT | Agent Interactions        2054
2017-07-31 16:38:42.515136 EDT | Epoch                       21
2017-07-31 16:38:42.515374 EDT | Iteration                   21
2017-07-31 16:38:42.515610 EDT | AverageReturn               97.9547
2017-07-31 16:38:42.515844 EDT | StdReturn                    6.12099
2017-07-31 16:38:42.516079 EDT | MaxReturn                  110.661
2017-07-31 16:38:42.516313 EDT | MinReturn                   88.7232
2017-07-31 16:38:42.516546 EDT | AverageEsReturn             17.5078
2017-07-31 16:38:42.516778 EDT | StdEsReturn                  1.79762
2017-07-31 16:38:42.517009 EDT | MaxEsReturn                 19.716
2017-07-31 16:38:42.517255 EDT | MinEsReturn                  5.41938
2017-07-31 16:38:42.517489 EDT | AverageDiscountedReturn     75.258
2017-07-31 16:38:42.517719 EDT | AverageQLoss                 0.211634
2017-07-31 16:38:42.517949 EDT | AveragePolicySurr           -8.29536
2017-07-31 16:38:42.518190 EDT | AverageQ                    13.7599
2017-07-31 16:38:42.518422 EDT | AverageAbsQ                 13.7604
2017-07-31 16:38:42.518654 EDT | AverageY                    13.7609
2017-07-31 16:38:42.518884 EDT | AverageAbsY                 13.7611
2017-07-31 16:38:42.519114 EDT | AverageAbsQYDiff             0.160408
2017-07-31 16:38:42.519344 EDT | AverageAction                0.928206
2017-07-31 16:38:42.519574 EDT | PolicyRegParamNorm          21.533
2017-07-31 16:38:42.519804 EDT | QFunRegParamNorm            21.473
2017-07-31 16:38:42.520034 EDT | -----------------------  ------------
2017-07-31 16:38:42.520454 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #22 | Training started
2017-07-31 16:39:11.609602 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #22 | Training finished
2017-07-31 16:39:11.610969 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #22 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:39:11.611292 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #22 | Collecting samples for evaluation
2017-07-31 16:39:30.256487 EDT | -----------------------  ------------
2017-07-31 16:39:30.259587 EDT | Oracle Interactions      20946
2017-07-31 16:39:30.259787 EDT | Agent Interactions        2054
2017-07-31 16:39:30.260029 EDT | Epoch                       22
2017-07-31 16:39:30.260209 EDT | Iteration                   22
2017-07-31 16:39:30.260416 EDT | AverageReturn                7.68186
2017-07-31 16:39:30.260588 EDT | StdReturn                    0.146514
2017-07-31 16:39:30.260751 EDT | MaxReturn                    9.01167
2017-07-31 16:39:30.260927 EDT | MinReturn                    6.41985
2017-07-31 16:39:30.261091 EDT | AverageEsReturn             17.6936
2017-07-31 16:39:30.261257 EDT | StdEsReturn                  2.1908
2017-07-31 16:39:30.261423 EDT | MaxEsReturn                 19.9252
2017-07-31 16:39:30.261582 EDT | MinEsReturn                  2.915
2017-07-31 16:39:30.261737 EDT | AverageDiscountedReturn      7.30997
2017-07-31 16:39:30.261913 EDT | AverageQLoss                 0.203223
2017-07-31 16:39:30.262100 EDT | AveragePolicySurr           -9.2255
2017-07-31 16:39:30.262295 EDT | AverageQ                    15.3544
2017-07-31 16:39:30.262453 EDT | AverageAbsQ                 15.3549
2017-07-31 16:39:30.262609 EDT | AverageY                    15.3555
2017-07-31 16:39:30.262773 EDT | AverageAbsY                 15.3556
2017-07-31 16:39:30.262926 EDT | AverageAbsQYDiff             0.16422
2017-07-31 16:39:30.263083 EDT | AverageAction                0.994603
2017-07-31 16:39:30.263271 EDT | PolicyRegParamNorm          21.8158
2017-07-31 16:39:30.263525 EDT | QFunRegParamNorm            22.2382
2017-07-31 16:39:30.263862 EDT | -----------------------  ------------
2017-07-31 16:39:30.264363 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #23 | Training started
2017-07-31 16:40:00.208795 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #23 | Training finished
2017-07-31 16:40:00.210078 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #23 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:40:00.210650 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #23 | Collecting samples for evaluation
2017-07-31 16:40:18.717165 EDT | -----------------------  ------------
2017-07-31 16:40:18.718173 EDT | Oracle Interactions      21946
2017-07-31 16:40:18.718483 EDT | Agent Interactions        2054
2017-07-31 16:40:18.718773 EDT | Epoch                       23
2017-07-31 16:40:18.719068 EDT | Iteration                   23
2017-07-31 16:40:18.719355 EDT | AverageReturn               41.1097
2017-07-31 16:40:18.719647 EDT | StdReturn                    7.68568
2017-07-31 16:40:18.719932 EDT | MaxReturn                   56.3099
2017-07-31 16:40:18.720232 EDT | MinReturn                   32.9972
2017-07-31 16:40:18.720524 EDT | AverageEsReturn             17.835
2017-07-31 16:40:18.720816 EDT | StdEsReturn                  1.09464
2017-07-31 16:40:18.721099 EDT | MaxEsReturn                 19.8088
2017-07-31 16:40:18.721383 EDT | MinEsReturn                 12.0407
2017-07-31 16:40:18.721667 EDT | AverageDiscountedReturn     35.9764
2017-07-31 16:40:18.721950 EDT | AverageQLoss                 0.285023
2017-07-31 16:40:18.722246 EDT | AveragePolicySurr          -10.2246
2017-07-31 16:40:18.722528 EDT | AverageQ                    17.144
2017-07-31 16:40:18.722814 EDT | AverageAbsQ                 17.1445
2017-07-31 16:40:18.723100 EDT | AverageY                    17.1452
2017-07-31 16:40:18.723381 EDT | AverageAbsY                 17.1454
2017-07-31 16:40:18.723662 EDT | AverageAbsQYDiff             0.174755
2017-07-31 16:40:18.723947 EDT | AverageAction                0.995813
2017-07-31 16:40:18.724232 EDT | PolicyRegParamNorm          22.1988
2017-07-31 16:40:18.724515 EDT | QFunRegParamNorm            22.8889
2017-07-31 16:40:18.724800 EDT | -----------------------  ------------
2017-07-31 16:40:18.725299 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #24 | Training started
2017-07-31 16:40:50.119023 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #24 | Training finished
2017-07-31 16:40:50.120120 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #24 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:40:50.120568 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #24 | Collecting samples for evaluation
2017-07-31 16:41:08.989293 EDT | -----------------------  ------------
2017-07-31 16:41:08.990260 EDT | Oracle Interactions      22946
2017-07-31 16:41:08.990529 EDT | Agent Interactions        2054
2017-07-31 16:41:08.990769 EDT | Epoch                       24
2017-07-31 16:41:08.991004 EDT | Iteration                   24
2017-07-31 16:41:08.991241 EDT | AverageReturn               53.3444
2017-07-31 16:41:08.991508 EDT | StdReturn                    0.632622
2017-07-31 16:41:08.991741 EDT | MaxReturn                   55.0764
2017-07-31 16:41:08.991971 EDT | MinReturn                   51.5867
2017-07-31 16:41:08.992200 EDT | AverageEsReturn             17.6788
2017-07-31 16:41:08.993004 EDT | StdEsReturn                  1.11076
2017-07-31 16:41:08.993246 EDT | MaxEsReturn                 19.7363
2017-07-31 16:41:08.993517 EDT | MinEsReturn                 12.0053
2017-07-31 16:41:08.993754 EDT | AverageDiscountedReturn     44.8536
2017-07-31 16:41:08.993987 EDT | AverageQLoss                 0.310702
2017-07-31 16:41:08.994232 EDT | AveragePolicySurr          -11.2159
2017-07-31 16:41:08.994492 EDT | AverageQ                    18.93
2017-07-31 16:41:08.994739 EDT | AverageAbsQ                 18.9307
2017-07-31 16:41:08.994974 EDT | AverageY                    18.9314
2017-07-31 16:41:08.995207 EDT | AverageAbsY                 18.9315
2017-07-31 16:41:08.995438 EDT | AverageAbsQYDiff             0.187082
2017-07-31 16:41:08.995708 EDT | AverageAction                0.999199
2017-07-31 16:41:08.995941 EDT | PolicyRegParamNorm          22.4831
2017-07-31 16:41:08.996181 EDT | QFunRegParamNorm            23.5896
2017-07-31 16:41:08.996411 EDT | -----------------------  ------------
2017-07-31 16:41:08.996835 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #25 | Training started
2017-07-31 16:41:40.898581 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #25 | Training finished
2017-07-31 16:41:40.899187 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #25 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:41:40.899427 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #25 | Collecting samples for evaluation
2017-07-31 16:41:58.977294 EDT | -----------------------  ------------
2017-07-31 16:41:58.978047 EDT | Oracle Interactions      23946
2017-07-31 16:41:58.978323 EDT | Agent Interactions        2054
2017-07-31 16:41:58.978569 EDT | Epoch                       25
2017-07-31 16:41:58.978814 EDT | Iteration                   25
2017-07-31 16:41:58.979048 EDT | AverageReturn               70.5796
2017-07-31 16:41:58.979286 EDT | StdReturn                    2.86527
2017-07-31 16:41:58.979518 EDT | MaxReturn                   77.3037
2017-07-31 16:41:58.979750 EDT | MinReturn                   59.9774
2017-07-31 16:41:58.979981 EDT | AverageEsReturn             17.738
2017-07-31 16:41:58.980211 EDT | StdEsReturn                  2.09045
2017-07-31 16:41:58.980440 EDT | MaxEsReturn                 19.8468
2017-07-31 16:41:58.980668 EDT | MinEsReturn                  3.56857
2017-07-31 16:41:58.980899 EDT | AverageDiscountedReturn     57.7818
2017-07-31 16:41:58.981127 EDT | AverageQLoss                 0.354725
2017-07-31 16:41:58.981355 EDT | AveragePolicySurr          -12.3021
2017-07-31 16:41:58.981584 EDT | AverageQ                    20.896
2017-07-31 16:41:58.981812 EDT | AverageAbsQ                 20.8967
2017-07-31 16:41:58.982040 EDT | AverageY                    20.8974
2017-07-31 16:41:58.982284 EDT | AverageAbsY                 20.8975
2017-07-31 16:41:58.982519 EDT | AverageAbsQYDiff             0.193755
2017-07-31 16:41:58.982749 EDT | AverageAction                0.96579
2017-07-31 16:41:58.982978 EDT | PolicyRegParamNorm          22.7945
2017-07-31 16:41:58.983210 EDT | QFunRegParamNorm            24.2563
2017-07-31 16:41:58.983439 EDT | -----------------------  ------------
2017-07-31 16:41:58.983863 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #26 | Training started
2017-07-31 16:42:29.776649 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #26 | Training finished
2017-07-31 16:42:29.777936 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #26 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:42:29.778266 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #26 | Collecting samples for evaluation
2017-07-31 16:42:47.607758 EDT | -----------------------  ------------
2017-07-31 16:42:47.608716 EDT | Oracle Interactions      24946
2017-07-31 16:42:47.609038 EDT | Agent Interactions        2054
2017-07-31 16:42:47.609330 EDT | Epoch                       26
2017-07-31 16:42:47.609604 EDT | Iteration                   26
2017-07-31 16:42:47.609875 EDT | AverageReturn               79.5648
2017-07-31 16:42:47.610173 EDT | StdReturn                    3.62956
2017-07-31 16:42:47.610451 EDT | MaxReturn                   85.9588
2017-07-31 16:42:47.610723 EDT | MinReturn                   67.0493
2017-07-31 16:42:47.610993 EDT | AverageEsReturn             17.681
2017-07-31 16:42:47.611263 EDT | StdEsReturn                  1.17767
2017-07-31 16:42:47.611531 EDT | MaxEsReturn                 19.7038
2017-07-31 16:42:47.611802 EDT | MinEsReturn                 11.2889
2017-07-31 16:42:47.612071 EDT | AverageDiscountedReturn     63.2881
2017-07-31 16:42:47.612339 EDT | AverageQLoss                 0.57992
2017-07-31 16:42:47.612607 EDT | AveragePolicySurr          -13.277
2017-07-31 16:42:47.612872 EDT | AverageQ                    22.7519
2017-07-31 16:42:47.613145 EDT | AverageAbsQ                 22.7528
2017-07-31 16:42:47.613415 EDT | AverageY                    22.7539
2017-07-31 16:42:47.613713 EDT | AverageAbsY                 22.7541
2017-07-31 16:42:47.614020 EDT | AverageAbsQYDiff             0.218776
2017-07-31 16:42:47.614382 EDT | AverageAction                0.967778
2017-07-31 16:42:47.614692 EDT | PolicyRegParamNorm          23.015
2017-07-31 16:42:47.615001 EDT | QFunRegParamNorm            24.9093
2017-07-31 16:42:47.615934 EDT | -----------------------  ------------
2017-07-31 16:42:47.616518 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #27 | Training started
2017-07-31 16:43:18.167021 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #27 | Training finished
2017-07-31 16:43:18.168434 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #27 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:43:18.169123 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #27 | Collecting samples for evaluation
2017-07-31 16:43:36.471058 EDT | -----------------------  ------------
2017-07-31 16:43:36.472057 EDT | Oracle Interactions      25946
2017-07-31 16:43:36.472368 EDT | Agent Interactions        2054
2017-07-31 16:43:36.472693 EDT | Epoch                       27
2017-07-31 16:43:36.473010 EDT | Iteration                   27
2017-07-31 16:43:36.473325 EDT | AverageReturn              136.751
2017-07-31 16:43:36.473638 EDT | StdReturn                   55.1976
2017-07-31 16:43:36.473928 EDT | MaxReturn                  181.502
2017-07-31 16:43:36.474240 EDT | MinReturn                   11.0706
2017-07-31 16:43:36.474517 EDT | AverageEsReturn             17.7885
2017-07-31 16:43:36.474789 EDT | StdEsReturn                  2.19138
2017-07-31 16:43:36.475060 EDT | MaxEsReturn                 19.9325
2017-07-31 16:43:36.475333 EDT | MinEsReturn                  3.11207
2017-07-31 16:43:36.475599 EDT | AverageDiscountedReturn     89.1078
2017-07-31 16:43:36.475866 EDT | AverageQLoss                 0.514732
2017-07-31 16:43:36.476136 EDT | AveragePolicySurr          -14.3254
2017-07-31 16:43:36.476403 EDT | AverageQ                    24.6766
2017-07-31 16:43:36.476670 EDT | AverageAbsQ                 24.6778
2017-07-31 16:43:36.476936 EDT | AverageY                    24.6776
2017-07-31 16:43:36.477202 EDT | AverageAbsY                 24.6776
2017-07-31 16:43:36.477472 EDT | AverageAbsQYDiff             0.216252
2017-07-31 16:43:36.477739 EDT | AverageAction                0.9407
2017-07-31 16:43:36.478010 EDT | PolicyRegParamNorm          23.0955
2017-07-31 16:43:36.478288 EDT | QFunRegParamNorm            25.4632
2017-07-31 16:43:36.478561 EDT | -----------------------  ------------
2017-07-31 16:43:36.479050 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #28 | Training started
2017-07-31 16:44:06.955330 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #28 | Training finished
2017-07-31 16:44:06.956316 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #28 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:44:06.956717 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #28 | Collecting samples for evaluation
2017-07-31 16:44:25.162512 EDT | -----------------------  ------------
2017-07-31 16:44:25.163006 EDT | Oracle Interactions      26946
2017-07-31 16:44:25.163268 EDT | Agent Interactions        2054
2017-07-31 16:44:25.163523 EDT | Epoch                       28
2017-07-31 16:44:25.163774 EDT | Iteration                   28
2017-07-31 16:44:25.164023 EDT | AverageReturn               76.5338
2017-07-31 16:44:25.164271 EDT | StdReturn                   61.9999
2017-07-31 16:44:25.164516 EDT | MaxReturn                  165.6
2017-07-31 16:44:25.164757 EDT | MinReturn                   11.0332
2017-07-31 16:44:25.165001 EDT | AverageEsReturn             17.7565
2017-07-31 16:44:25.165248 EDT | StdEsReturn                  1.40015
2017-07-31 16:44:25.165491 EDT | MaxEsReturn                 19.7921
2017-07-31 16:44:25.165736 EDT | MinEsReturn                  9.60107
2017-07-31 16:44:25.165979 EDT | AverageDiscountedReturn     53.2861
2017-07-31 16:44:25.166245 EDT | AverageQLoss                 0.669374
2017-07-31 16:44:25.166493 EDT | AveragePolicySurr          -15.5
2017-07-31 16:44:25.166742 EDT | AverageQ                    26.5825
2017-07-31 16:44:25.166987 EDT | AverageAbsQ                 26.5837
2017-07-31 16:44:25.167231 EDT | AverageY                    26.5841
2017-07-31 16:44:25.167474 EDT | AverageAbsY                 26.5841
2017-07-31 16:44:25.167717 EDT | AverageAbsQYDiff             0.233303
2017-07-31 16:44:25.167960 EDT | AverageAction                0.953144
2017-07-31 16:44:25.168204 EDT | PolicyRegParamNorm          23.2938
2017-07-31 16:44:25.168445 EDT | QFunRegParamNorm            26.1539
2017-07-31 16:44:25.168690 EDT | -----------------------  ------------
2017-07-31 16:44:25.169125 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #29 | Training started
2017-07-31 16:44:56.062414 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #29 | Training finished
2017-07-31 16:44:56.063219 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #29 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 16:44:56.063457 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_2] epoch #29 | Collecting samples for evaluation
2017-07-31 16:45:14.960885 EDT | -----------------------  -------------
2017-07-31 16:45:14.962042 EDT | Oracle Interactions      27946
2017-07-31 16:45:14.962339 EDT | Agent Interactions        2054
2017-07-31 16:45:14.962524 EDT | Epoch                       29
2017-07-31 16:45:14.962689 EDT | Iteration                   29
2017-07-31 16:45:14.962854 EDT | AverageReturn                5.42837
2017-07-31 16:45:14.963012 EDT | StdReturn                    0.0546341
2017-07-31 16:45:14.963177 EDT | MaxReturn                    5.5474
2017-07-31 16:45:14.963479 EDT | MinReturn                    5.09383
2017-07-31 16:45:14.963705 EDT | AverageEsReturn             17.9168
2017-07-31 16:45:14.963893 EDT | StdEsReturn                  0.831733
2017-07-31 16:45:14.964073 EDT | MaxEsReturn                 19.8946
2017-07-31 16:45:14.964239 EDT | MinEsReturn                 14.9221
2017-07-31 16:45:14.964399 EDT | AverageDiscountedReturn      5.25596
2017-07-31 16:45:14.964558 EDT | AverageQLoss                 0.79946
2017-07-31 16:45:14.964767 EDT | AveragePolicySurr          -16.5849
2017-07-31 16:45:14.964925 EDT | AverageQ                    28.7419
2017-07-31 16:45:14.965074 EDT | AverageAbsQ                 28.7433
2017-07-31 16:45:14.965222 EDT | AverageY                    28.7432
2017-07-31 16:45:14.965374 EDT | AverageAbsY                 28.7433
2017-07-31 16:45:14.965527 EDT | AverageAbsQYDiff             0.244924
2017-07-31 16:45:14.965673 EDT | AverageAction                0.992149
2017-07-31 16:45:14.965818 EDT | PolicyRegParamNorm          23.3019
2017-07-31 16:45:14.965964 EDT | QFunRegParamNorm            26.8735
2017-07-31 16:45:14.966108 EDT | -----------------------  -------------
