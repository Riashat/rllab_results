2017-07-31 15:31:32.626389 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] observation space: Box(11,)
2017-07-31 15:31:32.626989 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] action space: Box(3,)
2017-07-31 15:31:33.183832 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] Populating workers...
2017-07-31 15:31:33.184172 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] Populated
2017-07-31 15:31:36.132795 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #0 | Training started
2017-07-31 15:31:38.590322 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #0 | Training finished
2017-07-31 15:31:38.590753 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #0 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:38.591399 EDT | -------------------  ---
2017-07-31 15:31:38.591711 EDT | Oracle Interactions  397
2017-07-31 15:31:38.592000 EDT | Agent Interactions   603
2017-07-31 15:31:38.592288 EDT | -------------------  ---
2017-07-31 15:31:38.592716 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #1 | Training started
2017-07-31 15:31:41.351305 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #1 | Training finished
2017-07-31 15:31:41.352237 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #1 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:41.352985 EDT | -------------------  ----
2017-07-31 15:31:41.353298 EDT | Oracle Interactions   791
2017-07-31 15:31:41.353583 EDT | Agent Interactions   1209
2017-07-31 15:31:41.353863 EDT | -------------------  ----
2017-07-31 15:31:41.354287 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #2 | Training started
2017-07-31 15:31:43.953967 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #2 | Training finished
2017-07-31 15:31:43.954497 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #2 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:43.955204 EDT | -------------------  ----
2017-07-31 15:31:43.955540 EDT | Oracle Interactions  1179
2017-07-31 15:31:43.955837 EDT | Agent Interactions   1821
2017-07-31 15:31:43.956129 EDT | -------------------  ----
2017-07-31 15:31:43.956512 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #3 | Training started
2017-07-31 15:31:46.435391 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #3 | Training finished
2017-07-31 15:31:46.435797 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #3 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:46.436363 EDT | -------------------  ----
2017-07-31 15:31:46.436629 EDT | Oracle Interactions  1617
2017-07-31 15:31:46.436875 EDT | Agent Interactions   2383
2017-07-31 15:31:46.437116 EDT | -------------------  ----
2017-07-31 15:31:46.437439 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #4 | Training started
2017-07-31 15:31:49.071620 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #4 | Training finished
2017-07-31 15:31:49.072593 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #4 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:49.073310 EDT | -------------------  ----
2017-07-31 15:31:49.073693 EDT | Oracle Interactions  2011
2017-07-31 15:31:49.074041 EDT | Agent Interactions   2989
2017-07-31 15:31:49.074329 EDT | -------------------  ----
2017-07-31 15:31:49.074710 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #5 | Training started
2017-07-31 15:31:51.401955 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #5 | Training finished
2017-07-31 15:31:51.402398 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #5 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:51.403039 EDT | -------------------  ----
2017-07-31 15:31:51.403375 EDT | Oracle Interactions  2399
2017-07-31 15:31:51.403683 EDT | Agent Interactions   3601
2017-07-31 15:31:51.403987 EDT | -------------------  ----
2017-07-31 15:31:51.404386 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #6 | Training started
2017-07-31 15:31:53.680059 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #6 | Training finished
2017-07-31 15:31:53.680361 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #6 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:53.680828 EDT | -------------------  ----
2017-07-31 15:31:53.681017 EDT | Oracle Interactions  2726
2017-07-31 15:31:53.681191 EDT | Agent Interactions   4274
2017-07-31 15:31:53.681389 EDT | -------------------  ----
2017-07-31 15:31:53.681623 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #7 | Training started
2017-07-31 15:31:56.595405 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #7 | Training finished
2017-07-31 15:31:56.596208 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #7 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:56.596920 EDT | -------------------  ----
2017-07-31 15:31:56.597128 EDT | Oracle Interactions  3115
2017-07-31 15:31:56.597296 EDT | Agent Interactions   4885
2017-07-31 15:31:56.597515 EDT | -------------------  ----
2017-07-31 15:31:56.597771 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #8 | Training started
2017-07-31 15:31:59.078774 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #8 | Training finished
2017-07-31 15:31:59.079185 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #8 | Trained qf 0 steps, policy 0 steps
2017-07-31 15:31:59.079861 EDT | -------------------  ----
2017-07-31 15:31:59.080220 EDT | Oracle Interactions  3563
2017-07-31 15:31:59.080544 EDT | Agent Interactions   5437
2017-07-31 15:31:59.080862 EDT | -------------------  ----
2017-07-31 15:31:59.081290 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #9 | Training started
2017-07-31 15:32:02.050856 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #9 | Training finished
2017-07-31 15:32:02.051131 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #9 | Trained qf 1 steps, policy 1 steps
2017-07-31 15:32:02.051516 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #9 | Collecting samples for evaluation
2017-07-31 15:32:19.968495 EDT | -----------------------  ------------
2017-07-31 15:32:19.969405 EDT | Oracle Interactions      3956
2017-07-31 15:32:19.969676 EDT | Agent Interactions       6044
2017-07-31 15:32:19.969929 EDT | Epoch                       9
2017-07-31 15:32:19.970191 EDT | Iteration                   9
2017-07-31 15:32:19.970441 EDT | AverageReturn              20.3771
2017-07-31 15:32:19.970687 EDT | StdReturn                   0.483175
2017-07-31 15:32:19.970933 EDT | MaxReturn                  21.5595
2017-07-31 15:32:19.971200 EDT | MinReturn                  19.0527
2017-07-31 15:32:19.971488 EDT | AverageEsReturn            13.7714
2017-07-31 15:32:19.971731 EDT | StdEsReturn                14.1678
2017-07-31 15:32:19.971988 EDT | MaxEsReturn               185.684
2017-07-31 15:32:19.972303 EDT | MinEsReturn                 1.43589
2017-07-31 15:32:19.972595 EDT | AverageDiscountedReturn    17.9985
2017-07-31 15:32:19.972920 EDT | AverageQLoss                0.845922
2017-07-31 15:32:19.973255 EDT | AveragePolicySurr          -0.296585
2017-07-31 15:32:19.973586 EDT | AverageQ                    0.445729
2017-07-31 15:32:19.973908 EDT | AverageAbsQ                 0.445729
2017-07-31 15:32:19.974256 EDT | AverageY                    1.05023
2017-07-31 15:32:19.974586 EDT | AverageAbsY                 1.10218
2017-07-31 15:32:19.974924 EDT | AverageAbsQYDiff            0.799921
2017-07-31 15:32:19.975264 EDT | AverageAction               0.0039746
2017-07-31 15:32:19.975603 EDT | PolicyRegParamNorm         13.3355
2017-07-31 15:32:19.975943 EDT | QFunRegParamNorm           11.0408
2017-07-31 15:32:19.976286 EDT | -----------------------  ------------
2017-07-31 15:32:19.976810 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #10 | Training started
2017-07-31 15:32:51.051168 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #10 | Training finished
2017-07-31 15:32:51.052277 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #10 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:32:51.052706 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #10 | Collecting samples for evaluation
2017-07-31 15:33:10.626240 EDT | -----------------------  ------------
2017-07-31 15:33:10.627240 EDT | Oracle Interactions      4903
2017-07-31 15:33:10.627536 EDT | Agent Interactions       6097
2017-07-31 15:33:10.627824 EDT | Epoch                      10
2017-07-31 15:33:10.628102 EDT | Iteration                  10
2017-07-31 15:33:10.628373 EDT | AverageReturn               3.58846
2017-07-31 15:33:10.628641 EDT | StdReturn                   0.115529
2017-07-31 15:33:10.628907 EDT | MaxReturn                   3.87575
2017-07-31 15:33:10.629180 EDT | MinReturn                   3.29733
2017-07-31 15:33:10.629446 EDT | AverageEsReturn             7.39094
2017-07-31 15:33:10.629712 EDT | StdEsReturn                 1.05976
2017-07-31 15:33:10.629980 EDT | MaxEsReturn                 9.90454
2017-07-31 15:33:10.630256 EDT | MinEsReturn                 3.86887
2017-07-31 15:33:10.630522 EDT | AverageDiscountedReturn     3.53422
2017-07-31 15:33:10.630787 EDT | AverageQLoss                0.0298865
2017-07-31 15:33:10.631053 EDT | AveragePolicySurr          -1.10089
2017-07-31 15:33:10.631319 EDT | AverageQ                    1.36784
2017-07-31 15:33:10.631584 EDT | AverageAbsQ                 1.38169
2017-07-31 15:33:10.631855 EDT | AverageY                    1.37236
2017-07-31 15:33:10.632124 EDT | AverageAbsY                 1.39606
2017-07-31 15:33:10.632390 EDT | AverageAbsQYDiff            0.102677
2017-07-31 15:33:10.632655 EDT | AverageAction               0.999999
2017-07-31 15:33:10.632921 EDT | PolicyRegParamNorm         14.5574
2017-07-31 15:33:10.633189 EDT | QFunRegParamNorm           11.4947
2017-07-31 15:33:10.633452 EDT | -----------------------  ------------
2017-07-31 15:33:10.633981 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #11 | Training started
2017-07-31 15:33:41.593501 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #11 | Training finished
2017-07-31 15:33:41.594564 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #11 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:33:41.594954 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #11 | Collecting samples for evaluation
2017-07-31 15:34:01.664271 EDT | -----------------------  ------------
2017-07-31 15:34:01.667897 EDT | Oracle Interactions      5710
2017-07-31 15:34:01.668134 EDT | Agent Interactions       6290
2017-07-31 15:34:01.668314 EDT | Epoch                      11
2017-07-31 15:34:01.668481 EDT | Iteration                  11
2017-07-31 15:34:01.668781 EDT | AverageReturn               5.83823
2017-07-31 15:34:01.669121 EDT | StdReturn                   0.062575
2017-07-31 15:34:01.669311 EDT | MaxReturn                   6.00495
2017-07-31 15:34:01.669602 EDT | MinReturn                   5.69208
2017-07-31 15:34:01.669910 EDT | AverageEsReturn             6.27008
2017-07-31 15:34:01.670215 EDT | StdEsReturn                 1.8999
2017-07-31 15:34:01.670457 EDT | MaxEsReturn                10.3495
2017-07-31 15:34:01.670623 EDT | MinEsReturn                 1.29886
2017-07-31 15:34:01.670843 EDT | AverageDiscountedReturn     5.67945
2017-07-31 15:34:01.671045 EDT | AverageQLoss                0.02485
2017-07-31 15:34:01.671281 EDT | AveragePolicySurr          -1.47262
2017-07-31 15:34:01.671451 EDT | AverageQ                    1.9569
2017-07-31 15:34:01.671613 EDT | AverageAbsQ                 1.99393
2017-07-31 15:34:01.671891 EDT | AverageY                    1.95783
2017-07-31 15:34:01.672219 EDT | AverageAbsY                 1.99601
2017-07-31 15:34:01.672404 EDT | AverageAbsQYDiff            0.0880536
2017-07-31 15:34:01.672558 EDT | AverageAction               0.999999
2017-07-31 15:34:01.672709 EDT | PolicyRegParamNorm         15.7198
2017-07-31 15:34:01.672870 EDT | QFunRegParamNorm           12.0591
2017-07-31 15:34:01.673026 EDT | -----------------------  ------------
2017-07-31 15:34:01.673318 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #12 | Training started
2017-07-31 15:34:33.057637 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #12 | Training finished
2017-07-31 15:34:33.058489 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #12 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:34:33.058775 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #12 | Collecting samples for evaluation
2017-07-31 15:34:52.140756 EDT | -----------------------  ------------
2017-07-31 15:34:52.141837 EDT | Oracle Interactions      6229
2017-07-31 15:34:52.142189 EDT | Agent Interactions       6771
2017-07-31 15:34:52.142491 EDT | Epoch                      12
2017-07-31 15:34:52.142789 EDT | Iteration                  12
2017-07-31 15:34:52.143093 EDT | AverageReturn               5.83185
2017-07-31 15:34:52.143463 EDT | StdReturn                   0.0604915
2017-07-31 15:34:52.143790 EDT | MaxReturn                   6.00947
2017-07-31 15:34:52.144110 EDT | MinReturn                   5.69754
2017-07-31 15:34:52.144398 EDT | AverageEsReturn             7.89322
2017-07-31 15:34:52.144720 EDT | StdEsReturn                 2.17035
2017-07-31 15:34:52.145077 EDT | MaxEsReturn                13.5941
2017-07-31 15:34:52.145381 EDT | MinEsReturn                 5.38964
2017-07-31 15:34:52.145737 EDT | AverageDiscountedReturn     5.67316
2017-07-31 15:34:52.146030 EDT | AverageQLoss                0.0401325
2017-07-31 15:34:52.146382 EDT | AveragePolicySurr          -1.8502
2017-07-31 15:34:52.146688 EDT | AverageQ                    2.63322
2017-07-31 15:34:52.146989 EDT | AverageAbsQ                 2.67819
2017-07-31 15:34:52.147275 EDT | AverageY                    2.63441
2017-07-31 15:34:52.147598 EDT | AverageAbsY                 2.67974
2017-07-31 15:34:52.147948 EDT | AverageAbsQYDiff            0.107046
2017-07-31 15:34:52.148237 EDT | AverageAction               1
2017-07-31 15:34:52.148531 EDT | PolicyRegParamNorm         17.2902
2017-07-31 15:34:52.148879 EDT | QFunRegParamNorm           12.9235
2017-07-31 15:34:52.149204 EDT | -----------------------  ------------
2017-07-31 15:34:52.149713 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #13 | Training started
2017-07-31 15:35:23.208264 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #13 | Training finished
2017-07-31 15:35:23.219276 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #13 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:35:23.219803 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #13 | Collecting samples for evaluation
2017-07-31 15:35:42.529700 EDT | -----------------------  ------------
2017-07-31 15:35:42.530635 EDT | Oracle Interactions      6757
2017-07-31 15:35:42.530851 EDT | Agent Interactions       7243
2017-07-31 15:35:42.531028 EDT | Epoch                      13
2017-07-31 15:35:42.531250 EDT | Iteration                  13
2017-07-31 15:35:42.531470 EDT | AverageReturn               5.83659
2017-07-31 15:35:42.531684 EDT | StdReturn                   0.0604478
2017-07-31 15:35:42.531892 EDT | MaxReturn                   6.01646
2017-07-31 15:35:42.532103 EDT | MinReturn                   5.70266
2017-07-31 15:35:42.532339 EDT | AverageEsReturn             8.92653
2017-07-31 15:35:42.532565 EDT | StdEsReturn                 2.93775
2017-07-31 15:35:42.532797 EDT | MaxEsReturn                19.6048
2017-07-31 15:35:42.533020 EDT | MinEsReturn                 1.88757
2017-07-31 15:35:42.533240 EDT | AverageDiscountedReturn     5.67771
2017-07-31 15:35:42.533447 EDT | AverageQLoss                0.0524596
2017-07-31 15:35:42.533653 EDT | AveragePolicySurr          -2.14258
2017-07-31 15:35:42.533867 EDT | AverageQ                    3.303
2017-07-31 15:35:42.534073 EDT | AverageAbsQ                 3.35511
2017-07-31 15:35:42.534304 EDT | AverageY                    3.30338
2017-07-31 15:35:42.534528 EDT | AverageAbsY                 3.35609
2017-07-31 15:35:42.534692 EDT | AverageAbsQYDiff            0.122779
2017-07-31 15:35:42.534853 EDT | AverageAction               1
2017-07-31 15:35:42.535013 EDT | PolicyRegParamNorm         18.2932
2017-07-31 15:35:42.535192 EDT | QFunRegParamNorm           13.9536
2017-07-31 15:35:42.535370 EDT | -----------------------  ------------
2017-07-31 15:35:42.535690 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #14 | Training started
2017-07-31 15:36:13.266871 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #14 | Training finished
2017-07-31 15:36:13.267801 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #14 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:36:13.268111 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #14 | Collecting samples for evaluation
2017-07-31 15:36:32.809737 EDT | -----------------------  ------------
2017-07-31 15:36:32.810751 EDT | Oracle Interactions      7259
2017-07-31 15:36:32.811254 EDT | Agent Interactions       7741
2017-07-31 15:36:32.811753 EDT | Epoch                      14
2017-07-31 15:36:32.812223 EDT | Iteration                  14
2017-07-31 15:36:32.812701 EDT | AverageReturn               5.83561
2017-07-31 15:36:32.813171 EDT | StdReturn                   0.0621031
2017-07-31 15:36:32.813622 EDT | MaxReturn                   6.00192
2017-07-31 15:36:32.814084 EDT | MinReturn                   5.70274
2017-07-31 15:36:32.814553 EDT | AverageEsReturn             9.30405
2017-07-31 15:36:32.815009 EDT | StdEsReturn                 3.93676
2017-07-31 15:36:32.815474 EDT | MaxEsReturn                25.8704
2017-07-31 15:36:32.815919 EDT | MinEsReturn                 2.72441
2017-07-31 15:36:32.816383 EDT | AverageDiscountedReturn     5.67677
2017-07-31 15:36:32.816831 EDT | AverageQLoss                0.0585421
2017-07-31 15:36:32.817299 EDT | AveragePolicySurr          -2.36162
2017-07-31 15:36:32.817741 EDT | AverageQ                    3.83879
2017-07-31 15:36:32.818206 EDT | AverageAbsQ                 3.89117
2017-07-31 15:36:32.818651 EDT | AverageY                    3.83938
2017-07-31 15:36:32.819096 EDT | AverageAbsY                 3.89258
2017-07-31 15:36:32.819558 EDT | AverageAbsQYDiff            0.127856
2017-07-31 15:36:32.820002 EDT | AverageAction               1
2017-07-31 15:36:32.820450 EDT | PolicyRegParamNorm         19.2147
2017-07-31 15:36:32.820896 EDT | QFunRegParamNorm           15.008
2017-07-31 15:36:32.821339 EDT | -----------------------  ------------
2017-07-31 15:36:32.822023 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #15 | Training started
2017-07-31 15:37:03.540804 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #15 | Training finished
2017-07-31 15:37:03.541629 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #15 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:37:03.541993 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #15 | Collecting samples for evaluation
2017-07-31 15:37:21.942409 EDT | -----------------------  ------------
2017-07-31 15:37:21.945890 EDT | Oracle Interactions      7644
2017-07-31 15:37:21.946186 EDT | Agent Interactions       8356
2017-07-31 15:37:21.946421 EDT | Epoch                      15
2017-07-31 15:37:21.946654 EDT | Iteration                  15
2017-07-31 15:37:21.946879 EDT | AverageReturn               5.83572
2017-07-31 15:37:21.947081 EDT | StdReturn                   0.0613417
2017-07-31 15:37:21.947296 EDT | MaxReturn                   6.03195
2017-07-31 15:37:21.947539 EDT | MinReturn                   5.69619
2017-07-31 15:37:21.947749 EDT | AverageEsReturn             9.33746
2017-07-31 15:37:21.948106 EDT | StdEsReturn                 4.58342
2017-07-31 15:37:21.948363 EDT | MaxEsReturn                40.7211
2017-07-31 15:37:21.948539 EDT | MinEsReturn                 5.3908
2017-07-31 15:37:21.948758 EDT | AverageDiscountedReturn     5.67695
2017-07-31 15:37:21.949112 EDT | AverageQLoss                0.0628887
2017-07-31 15:37:21.949293 EDT | AveragePolicySurr          -2.51094
2017-07-31 15:37:21.949478 EDT | AverageQ                    4.21799
2017-07-31 15:37:21.949682 EDT | AverageAbsQ                 4.2723
2017-07-31 15:37:21.949910 EDT | AverageY                    4.21819
2017-07-31 15:37:21.950072 EDT | AverageAbsY                 4.27368
2017-07-31 15:37:21.950274 EDT | AverageAbsQYDiff            0.137325
2017-07-31 15:37:21.950616 EDT | AverageAction               1
2017-07-31 15:37:21.950980 EDT | PolicyRegParamNorm         19.5629
2017-07-31 15:37:21.951216 EDT | QFunRegParamNorm           16.1422
2017-07-31 15:37:21.951383 EDT | -----------------------  ------------
2017-07-31 15:37:21.951763 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #16 | Training started
2017-07-31 15:37:51.738526 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #16 | Training finished
2017-07-31 15:37:51.739239 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #16 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:37:51.739468 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #16 | Collecting samples for evaluation
2017-07-31 15:38:10.629082 EDT | -----------------------  ------------
2017-07-31 15:38:10.629888 EDT | Oracle Interactions      8255
2017-07-31 15:38:10.630120 EDT | Agent Interactions       8745
2017-07-31 15:38:10.630305 EDT | Epoch                      16
2017-07-31 15:38:10.630467 EDT | Iteration                  16
2017-07-31 15:38:10.630617 EDT | AverageReturn               5.83658
2017-07-31 15:38:10.630780 EDT | StdReturn                   0.0614752
2017-07-31 15:38:10.631026 EDT | MaxReturn                   6.03733
2017-07-31 15:38:10.631331 EDT | MinReturn                   5.69837
2017-07-31 15:38:10.631495 EDT | AverageEsReturn            10.9091
2017-07-31 15:38:10.631651 EDT | StdEsReturn                 3.59794
2017-07-31 15:38:10.631802 EDT | MaxEsReturn                23.972
2017-07-31 15:38:10.631963 EDT | MinEsReturn                 5.49635
2017-07-31 15:38:10.632117 EDT | AverageDiscountedReturn     5.67781
2017-07-31 15:38:10.632271 EDT | AverageQLoss                0.0625822
2017-07-31 15:38:10.632540 EDT | AveragePolicySurr          -2.57886
2017-07-31 15:38:10.632852 EDT | AverageQ                    4.46746
2017-07-31 15:38:10.633023 EDT | AverageAbsQ                 4.5224
2017-07-31 15:38:10.633171 EDT | AverageY                    4.46798
2017-07-31 15:38:10.633329 EDT | AverageAbsY                 4.52402
2017-07-31 15:38:10.633474 EDT | AverageAbsQYDiff            0.144298
2017-07-31 15:38:10.633627 EDT | AverageAction               1
2017-07-31 15:38:10.633893 EDT | PolicyRegParamNorm         20.2007
2017-07-31 15:38:10.634052 EDT | QFunRegParamNorm           17.3095
2017-07-31 15:38:10.634233 EDT | -----------------------  ------------
2017-07-31 15:38:10.634507 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #17 | Training started
2017-07-31 15:38:40.738972 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #17 | Training finished
2017-07-31 15:38:40.739910 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #17 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:38:40.740255 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #17 | Collecting samples for evaluation
2017-07-31 15:38:59.071136 EDT | -----------------------  ------------
2017-07-31 15:38:59.071971 EDT | Oracle Interactions      8433
2017-07-31 15:38:59.072163 EDT | Agent Interactions       9567
2017-07-31 15:38:59.072329 EDT | Epoch                      17
2017-07-31 15:38:59.072485 EDT | Iteration                  17
2017-07-31 15:38:59.072644 EDT | AverageReturn               5.83582
2017-07-31 15:38:59.072806 EDT | StdReturn                   0.0594628
2017-07-31 15:38:59.072954 EDT | MaxReturn                   6.00783
2017-07-31 15:38:59.073113 EDT | MinReturn                   5.69438
2017-07-31 15:38:59.073363 EDT | AverageEsReturn             8.4022
2017-07-31 15:38:59.073576 EDT | StdEsReturn                 3.60577
2017-07-31 15:38:59.073728 EDT | MaxEsReturn                26.5765
2017-07-31 15:38:59.073962 EDT | MinEsReturn                 5.40612
2017-07-31 15:38:59.074112 EDT | AverageDiscountedReturn     5.67705
2017-07-31 15:38:59.074343 EDT | AverageQLoss                0.0580165
2017-07-31 15:38:59.074499 EDT | AveragePolicySurr          -2.55385
2017-07-31 15:38:59.074654 EDT | AverageQ                    4.54489
2017-07-31 15:38:59.074813 EDT | AverageAbsQ                 4.6003
2017-07-31 15:38:59.074968 EDT | AverageY                    4.545
2017-07-31 15:38:59.075122 EDT | AverageAbsY                 4.60083
2017-07-31 15:38:59.075286 EDT | AverageAbsQYDiff            0.139984
2017-07-31 15:38:59.075504 EDT | AverageAction               1
2017-07-31 15:38:59.075667 EDT | PolicyRegParamNorm         20.4455
2017-07-31 15:38:59.075846 EDT | QFunRegParamNorm           18.2707
2017-07-31 15:38:59.076045 EDT | -----------------------  ------------
2017-07-31 15:38:59.076488 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #18 | Training started
2017-07-31 15:39:28.364562 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #18 | Training finished
2017-07-31 15:39:28.365384 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #18 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:39:28.365603 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #18 | Collecting samples for evaluation
2017-07-31 15:39:47.691704 EDT | -----------------------  -------------
2017-07-31 15:39:47.692497 EDT | Oracle Interactions       8594
2017-07-31 15:39:47.692698 EDT | Agent Interactions       10406
2017-07-31 15:39:47.692889 EDT | Epoch                       18
2017-07-31 15:39:47.693065 EDT | Iteration                   18
2017-07-31 15:39:47.693232 EDT | AverageReturn                5.83989
2017-07-31 15:39:47.693403 EDT | StdReturn                    0.0621838
2017-07-31 15:39:47.693720 EDT | MaxReturn                    6.01131
2017-07-31 15:39:47.693912 EDT | MinReturn                    5.69417
2017-07-31 15:39:47.694073 EDT | AverageEsReturn              9.88431
2017-07-31 15:39:47.694270 EDT | StdEsReturn                  8.42728
2017-07-31 15:39:47.694429 EDT | MaxEsReturn                 67.3097
2017-07-31 15:39:47.694580 EDT | MinEsReturn                  5.47497
2017-07-31 15:39:47.694735 EDT | AverageDiscountedReturn      5.68104
2017-07-31 15:39:47.694894 EDT | AverageQLoss                 0.0743309
2017-07-31 15:39:47.695050 EDT | AveragePolicySurr           -2.50685
2017-07-31 15:39:47.695213 EDT | AverageQ                     4.54779
2017-07-31 15:39:47.695422 EDT | AverageAbsQ                  4.60398
2017-07-31 15:39:47.695578 EDT | AverageY                     4.54824
2017-07-31 15:39:47.695737 EDT | AverageAbsY                  4.60499
2017-07-31 15:39:47.695888 EDT | AverageAbsQYDiff             0.152474
2017-07-31 15:39:47.696054 EDT | AverageAction                1
2017-07-31 15:39:47.696359 EDT | PolicyRegParamNorm          20.5595
2017-07-31 15:39:47.696539 EDT | QFunRegParamNorm            19.332
2017-07-31 15:39:47.696789 EDT | -----------------------  -------------
2017-07-31 15:39:47.697127 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #19 | Training started
2017-07-31 15:40:17.828163 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #19 | Training finished
2017-07-31 15:40:17.829080 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #19 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:40:17.829494 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #19 | Collecting samples for evaluation
2017-07-31 15:40:37.514271 EDT | -----------------------  -------------
2017-07-31 15:40:37.515130 EDT | Oracle Interactions       8834
2017-07-31 15:40:37.515380 EDT | Agent Interactions       11166
2017-07-31 15:40:37.515555 EDT | Epoch                       19
2017-07-31 15:40:37.515727 EDT | Iteration                   19
2017-07-31 15:40:37.515897 EDT | AverageReturn                5.83526
2017-07-31 15:40:37.516140 EDT | StdReturn                    0.060738
2017-07-31 15:40:37.516336 EDT | MaxReturn                    6.02905
2017-07-31 15:40:37.516541 EDT | MinReturn                    5.70222
2017-07-31 15:40:37.516729 EDT | AverageEsReturn              8.82484
2017-07-31 15:40:37.517012 EDT | StdEsReturn                  3.73144
2017-07-31 15:40:37.517336 EDT | MaxEsReturn                 35.5374
2017-07-31 15:40:37.517504 EDT | MinEsReturn                  5.32484
2017-07-31 15:40:37.517658 EDT | AverageDiscountedReturn      5.67649
2017-07-31 15:40:37.517806 EDT | AverageQLoss                 0.0838751
2017-07-31 15:40:37.517952 EDT | AveragePolicySurr           -2.42961
2017-07-31 15:40:37.518108 EDT | AverageQ                     4.42997
2017-07-31 15:40:37.518276 EDT | AverageAbsQ                  4.48761
2017-07-31 15:40:37.518430 EDT | AverageY                     4.43075
2017-07-31 15:40:37.518580 EDT | AverageAbsY                  4.4887
2017-07-31 15:40:37.518724 EDT | AverageAbsQYDiff             0.159703
2017-07-31 15:40:37.518868 EDT | AverageAction                1
2017-07-31 15:40:37.519012 EDT | PolicyRegParamNorm          20.8216
2017-07-31 15:40:37.519156 EDT | QFunRegParamNorm            20.2884
2017-07-31 15:40:37.519298 EDT | -----------------------  -------------
2017-07-31 15:40:37.519566 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #20 | Training started
2017-07-31 15:41:07.392359 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #20 | Training finished
2017-07-31 15:41:07.393241 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #20 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:41:07.393460 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #20 | Collecting samples for evaluation
2017-07-31 15:41:26.536226 EDT | -----------------------  -------------
2017-07-31 15:41:26.537198 EDT | Oracle Interactions       9475
2017-07-31 15:41:26.537462 EDT | Agent Interactions       11525
2017-07-31 15:41:26.537710 EDT | Epoch                       20
2017-07-31 15:41:26.537953 EDT | Iteration                   20
2017-07-31 15:41:26.538204 EDT | AverageReturn                5.83512
2017-07-31 15:41:26.538444 EDT | StdReturn                    0.0617282
2017-07-31 15:41:26.538685 EDT | MaxReturn                    6.0175
2017-07-31 15:41:26.538926 EDT | MinReturn                    5.70112
2017-07-31 15:41:26.539163 EDT | AverageEsReturn             11.1223
2017-07-31 15:41:26.539400 EDT | StdEsReturn                  4.5853
2017-07-31 15:41:26.539644 EDT | MaxEsReturn                 29.7111
2017-07-31 15:41:26.539884 EDT | MinEsReturn                  5.46415
2017-07-31 15:41:26.540124 EDT | AverageDiscountedReturn      5.67639
2017-07-31 15:41:26.540360 EDT | AverageQLoss                 0.0653151
2017-07-31 15:41:26.540597 EDT | AveragePolicySurr           -2.37671
2017-07-31 15:41:26.540838 EDT | AverageQ                     4.29848
2017-07-31 15:41:26.541070 EDT | AverageAbsQ                  4.36358
2017-07-31 15:41:26.541302 EDT | AverageY                     4.29863
2017-07-31 15:41:26.541532 EDT | AverageAbsY                  4.36407
2017-07-31 15:41:26.541767 EDT | AverageAbsQYDiff             0.149967
2017-07-31 15:41:26.541998 EDT | AverageAction                1
2017-07-31 15:41:26.542248 EDT | PolicyRegParamNorm          21.0417
2017-07-31 15:41:26.542485 EDT | QFunRegParamNorm            21.0706
2017-07-31 15:41:26.542721 EDT | -----------------------  -------------
2017-07-31 15:41:26.543154 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #21 | Training started
2017-07-31 15:41:55.703004 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #21 | Training finished
2017-07-31 15:41:55.703995 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #21 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:41:55.704343 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #21 | Collecting samples for evaluation
2017-07-31 15:42:14.616370 EDT | -----------------------  -------------
2017-07-31 15:42:14.617972 EDT | Oracle Interactions       9583
2017-07-31 15:42:14.619032 EDT | Agent Interactions       12417
2017-07-31 15:42:14.619677 EDT | Epoch                       21
2017-07-31 15:42:14.620268 EDT | Iteration                   21
2017-07-31 15:42:14.620879 EDT | AverageReturn                5.83807
2017-07-31 15:42:14.621458 EDT | StdReturn                    0.0618648
2017-07-31 15:42:14.621966 EDT | MaxReturn                    6.0184
2017-07-31 15:42:14.622319 EDT | MinReturn                    5.70456
2017-07-31 15:42:14.622663 EDT | AverageEsReturn              7.68801
2017-07-31 15:42:14.622940 EDT | StdEsReturn                  2.62948
2017-07-31 15:42:14.623276 EDT | MaxEsReturn                 24.5423
2017-07-31 15:42:14.623637 EDT | MinEsReturn                  5.37771
2017-07-31 15:42:14.623940 EDT | AverageDiscountedReturn      5.67918
2017-07-31 15:42:14.624201 EDT | AverageQLoss                 0.0668035
2017-07-31 15:42:14.624445 EDT | AveragePolicySurr           -2.38344
2017-07-31 15:42:14.624688 EDT | AverageQ                     4.22988
2017-07-31 15:42:14.624921 EDT | AverageAbsQ                  4.29885
2017-07-31 15:42:14.625148 EDT | AverageY                     4.23046
2017-07-31 15:42:14.625372 EDT | AverageAbsY                  4.29981
2017-07-31 15:42:14.625595 EDT | AverageAbsQYDiff             0.148855
2017-07-31 15:42:14.625824 EDT | AverageAction                1
2017-07-31 15:42:14.626090 EDT | PolicyRegParamNorm          21.3082
2017-07-31 15:42:14.626330 EDT | QFunRegParamNorm            21.7569
2017-07-31 15:42:14.626555 EDT | -----------------------  -------------
2017-07-31 15:42:14.627100 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #22 | Training started
2017-07-31 15:42:44.613353 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #22 | Training finished
2017-07-31 15:42:44.614317 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #22 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:42:44.614651 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #22 | Collecting samples for evaluation
2017-07-31 15:43:04.098050 EDT | -----------------------  -------------
2017-07-31 15:43:04.099006 EDT | Oracle Interactions       9808
2017-07-31 15:43:04.099258 EDT | Agent Interactions       13192
2017-07-31 15:43:04.099485 EDT | Epoch                       22
2017-07-31 15:43:04.099694 EDT | Iteration                   22
2017-07-31 15:43:04.099931 EDT | AverageReturn                5.8362
2017-07-31 15:43:04.100130 EDT | StdReturn                    0.0613802
2017-07-31 15:43:04.100420 EDT | MaxReturn                    6.02078
2017-07-31 15:43:04.100613 EDT | MinReturn                    5.69532
2017-07-31 15:43:04.100807 EDT | AverageEsReturn              8.75652
2017-07-31 15:43:04.100982 EDT | StdEsReturn                  4.01056
2017-07-31 15:43:04.101165 EDT | MaxEsReturn                 38.4166
2017-07-31 15:43:04.101337 EDT | MinEsReturn                  5.59474
2017-07-31 15:43:04.101505 EDT | AverageDiscountedReturn      5.67747
2017-07-31 15:43:04.101674 EDT | AverageQLoss                 0.0566523
2017-07-31 15:43:04.101848 EDT | AveragePolicySurr           -2.40381
2017-07-31 15:43:04.102016 EDT | AverageQ                     4.22746
2017-07-31 15:43:04.102259 EDT | AverageAbsQ                  4.29513
2017-07-31 15:43:04.102529 EDT | AverageY                     4.22804
2017-07-31 15:43:04.102858 EDT | AverageAbsY                  4.29576
2017-07-31 15:43:04.103202 EDT | AverageAbsQYDiff             0.139329
2017-07-31 15:43:04.103413 EDT | AverageAction                1
2017-07-31 15:43:04.103578 EDT | PolicyRegParamNorm          21.5014
2017-07-31 15:43:04.103765 EDT | QFunRegParamNorm            22.3239
2017-07-31 15:43:04.103915 EDT | -----------------------  -------------
2017-07-31 15:43:04.104221 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #23 | Training started
2017-07-31 15:43:33.802395 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #23 | Training finished
2017-07-31 15:43:33.803282 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #23 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:43:33.803519 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #23 | Collecting samples for evaluation
2017-07-31 15:43:53.597873 EDT | -----------------------  -------------
2017-07-31 15:43:53.599330 EDT | Oracle Interactions       9965
2017-07-31 15:43:53.599843 EDT | Agent Interactions       14035
2017-07-31 15:43:53.600192 EDT | Epoch                       23
2017-07-31 15:43:53.600500 EDT | Iteration                   23
2017-07-31 15:43:53.600673 EDT | AverageReturn                5.83808
2017-07-31 15:43:53.600837 EDT | StdReturn                    0.0608657
2017-07-31 15:43:53.601031 EDT | MaxReturn                    6.02471
2017-07-31 15:43:53.601286 EDT | MinReturn                    5.69848
2017-07-31 15:43:53.601441 EDT | AverageEsReturn              8.84454
2017-07-31 15:43:53.601609 EDT | StdEsReturn                  4.2175
2017-07-31 15:43:53.601786 EDT | MaxEsReturn                 30.9848
2017-07-31 15:43:53.601962 EDT | MinEsReturn                  5.32669
2017-07-31 15:43:53.602242 EDT | AverageDiscountedReturn      5.6793
2017-07-31 15:43:53.602469 EDT | AverageQLoss                 0.0569343
2017-07-31 15:43:53.602754 EDT | AveragePolicySurr           -2.41763
2017-07-31 15:43:53.603033 EDT | AverageQ                     4.22526
2017-07-31 15:43:53.603315 EDT | AverageAbsQ                  4.29169
2017-07-31 15:43:53.603669 EDT | AverageY                     4.22593
2017-07-31 15:43:53.604007 EDT | AverageAbsY                  4.29241
2017-07-31 15:43:53.604229 EDT | AverageAbsQYDiff             0.137177
2017-07-31 15:43:53.604424 EDT | AverageAction                1
2017-07-31 15:43:53.604732 EDT | PolicyRegParamNorm          21.9405
2017-07-31 15:43:53.605049 EDT | QFunRegParamNorm            22.8738
2017-07-31 15:43:53.605335 EDT | -----------------------  -------------
2017-07-31 15:43:53.605805 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #24 | Training started
2017-07-31 15:44:22.940084 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #24 | Training finished
2017-07-31 15:44:22.941645 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #24 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:44:22.941902 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #24 | Collecting samples for evaluation
2017-07-31 15:44:41.794608 EDT | -----------------------  -------------
2017-07-31 15:44:41.795487 EDT | Oracle Interactions      10038
2017-07-31 15:44:41.795830 EDT | Agent Interactions       14962
2017-07-31 15:44:41.796097 EDT | Epoch                       24
2017-07-31 15:44:41.796453 EDT | Iteration                   24
2017-07-31 15:44:41.796676 EDT | AverageReturn                5.83775
2017-07-31 15:44:41.796897 EDT | StdReturn                    0.0631446
2017-07-31 15:44:41.797129 EDT | MaxReturn                    6.0169
2017-07-31 15:44:41.797283 EDT | MinReturn                    5.70187
2017-07-31 15:44:41.797451 EDT | AverageEsReturn              7.42234
2017-07-31 15:44:41.797651 EDT | StdEsReturn                  2.72089
2017-07-31 15:44:41.797806 EDT | MaxEsReturn                 21.2037
2017-07-31 15:44:41.797970 EDT | MinEsReturn                  5.23612
2017-07-31 15:44:41.798117 EDT | AverageDiscountedReturn      5.679
2017-07-31 15:44:41.798310 EDT | AverageQLoss                 0.0492216
2017-07-31 15:44:41.798465 EDT | AveragePolicySurr           -2.41534
2017-07-31 15:44:41.798615 EDT | AverageQ                     4.2181
2017-07-31 15:44:41.798759 EDT | AverageAbsQ                  4.2784
2017-07-31 15:44:41.798914 EDT | AverageY                     4.21864
2017-07-31 15:44:41.799063 EDT | AverageAbsY                  4.27901
2017-07-31 15:44:41.799212 EDT | AverageAbsQYDiff             0.127893
2017-07-31 15:44:41.799381 EDT | AverageAction                1
2017-07-31 15:44:41.799530 EDT | PolicyRegParamNorm          22.0043
2017-07-31 15:44:41.799696 EDT | QFunRegParamNorm            23.3194
2017-07-31 15:44:41.799844 EDT | -----------------------  -------------
2017-07-31 15:44:41.800217 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #25 | Training started
2017-07-31 15:45:12.472049 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #25 | Training finished
2017-07-31 15:45:12.473700 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #25 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:45:12.474058 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #25 | Collecting samples for evaluation
2017-07-31 15:45:30.818117 EDT | -----------------------  -------------
2017-07-31 15:45:30.819111 EDT | Oracle Interactions      10140
2017-07-31 15:45:30.819470 EDT | Agent Interactions       15860
2017-07-31 15:45:30.819802 EDT | Epoch                       25
2017-07-31 15:45:30.820125 EDT | Iteration                   25
2017-07-31 15:45:30.820455 EDT | AverageReturn                5.83385
2017-07-31 15:45:30.820686 EDT | StdReturn                    0.0605304
2017-07-31 15:45:30.820928 EDT | MaxReturn                    6.02354
2017-07-31 15:45:30.821217 EDT | MinReturn                    5.6971
2017-07-31 15:45:30.821506 EDT | AverageEsReturn              7.63141
2017-07-31 15:45:30.821781 EDT | StdEsReturn                  2.47608
2017-07-31 15:45:30.822013 EDT | MaxEsReturn                 19.6211
2017-07-31 15:45:30.822384 EDT | MinEsReturn                  5.47262
2017-07-31 15:45:30.822661 EDT | AverageDiscountedReturn      5.67508
2017-07-31 15:45:30.822856 EDT | AverageQLoss                 0.0524914
2017-07-31 15:45:30.823129 EDT | AveragePolicySurr           -2.43204
2017-07-31 15:45:30.823413 EDT | AverageQ                     4.22559
2017-07-31 15:45:30.823673 EDT | AverageAbsQ                  4.27959
2017-07-31 15:45:30.823989 EDT | AverageY                     4.22651
2017-07-31 15:45:30.824255 EDT | AverageAbsY                  4.28058
2017-07-31 15:45:30.824514 EDT | AverageAbsQYDiff             0.131387
2017-07-31 15:45:30.824836 EDT | AverageAction                1
2017-07-31 15:45:30.825079 EDT | PolicyRegParamNorm          22.1635
2017-07-31 15:45:30.825425 EDT | QFunRegParamNorm            23.7794
2017-07-31 15:45:30.825593 EDT | -----------------------  -------------
2017-07-31 15:45:30.826093 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #26 | Training started
2017-07-31 15:46:00.565551 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #26 | Training finished
2017-07-31 15:46:00.566417 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #26 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:46:00.566783 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #26 | Collecting samples for evaluation
2017-07-31 15:46:19.827496 EDT | -----------------------  -------------
2017-07-31 15:46:19.828435 EDT | Oracle Interactions      10301
2017-07-31 15:46:19.828726 EDT | Agent Interactions       16699
2017-07-31 15:46:19.829036 EDT | Epoch                       26
2017-07-31 15:46:19.829206 EDT | Iteration                   26
2017-07-31 15:46:19.829376 EDT | AverageReturn                5.83188
2017-07-31 15:46:19.829541 EDT | StdReturn                    0.0603315
2017-07-31 15:46:19.829697 EDT | MaxReturn                    6.01412
2017-07-31 15:46:19.829855 EDT | MinReturn                    5.69121
2017-07-31 15:46:19.830008 EDT | AverageEsReturn              8.16357
2017-07-31 15:46:19.830177 EDT | StdEsReturn                  3.51261
2017-07-31 15:46:19.830334 EDT | MaxEsReturn                 26.6766
2017-07-31 15:46:19.830483 EDT | MinEsReturn                  5.28372
2017-07-31 15:46:19.830635 EDT | AverageDiscountedReturn      5.67308
2017-07-31 15:46:19.830797 EDT | AverageQLoss                 0.0526856
2017-07-31 15:46:19.831013 EDT | AveragePolicySurr           -2.4194
2017-07-31 15:46:19.831229 EDT | AverageQ                     4.19749
2017-07-31 15:46:19.831450 EDT | AverageAbsQ                  4.24501
2017-07-31 15:46:19.831611 EDT | AverageY                     4.19821
2017-07-31 15:46:19.831769 EDT | AverageAbsY                  4.24578
2017-07-31 15:46:19.831925 EDT | AverageAbsQYDiff             0.126648
2017-07-31 15:46:19.832084 EDT | AverageAction                1
2017-07-31 15:46:19.832362 EDT | PolicyRegParamNorm          22.3174
2017-07-31 15:46:19.832638 EDT | QFunRegParamNorm            24.1787
2017-07-31 15:46:19.832808 EDT | -----------------------  -------------
2017-07-31 15:46:19.833214 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #27 | Training started
2017-07-31 15:46:49.852115 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #27 | Training finished
2017-07-31 15:46:49.853053 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #27 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:46:49.853401 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #27 | Collecting samples for evaluation
2017-07-31 15:47:09.422131 EDT | -----------------------  -------------
2017-07-31 15:47:09.423056 EDT | Oracle Interactions      10461
2017-07-31 15:47:09.423333 EDT | Agent Interactions       17539
2017-07-31 15:47:09.423604 EDT | Epoch                       27
2017-07-31 15:47:09.423860 EDT | Iteration                   27
2017-07-31 15:47:09.424103 EDT | AverageReturn                5.8369
2017-07-31 15:47:09.424347 EDT | StdReturn                    0.0608799
2017-07-31 15:47:09.424588 EDT | MaxReturn                    6.00893
2017-07-31 15:47:09.424829 EDT | MinReturn                    5.69262
2017-07-31 15:47:09.425073 EDT | AverageEsReturn              8.68493
2017-07-31 15:47:09.425313 EDT | StdEsReturn                  5.75636
2017-07-31 15:47:09.425558 EDT | MaxEsReturn                 47.3301
2017-07-31 15:47:09.425809 EDT | MinEsReturn                  5.2628
2017-07-31 15:47:09.426050 EDT | AverageDiscountedReturn      5.67809
2017-07-31 15:47:09.426316 EDT | AverageQLoss                 0.0440034
2017-07-31 15:47:09.426560 EDT | AveragePolicySurr           -2.44343
2017-07-31 15:47:09.426802 EDT | AverageQ                     4.22149
2017-07-31 15:47:09.427043 EDT | AverageAbsQ                  4.26226
2017-07-31 15:47:09.427290 EDT | AverageY                     4.22206
2017-07-31 15:47:09.427530 EDT | AverageAbsY                  4.26284
2017-07-31 15:47:09.427778 EDT | AverageAbsQYDiff             0.121483
2017-07-31 15:47:09.428018 EDT | AverageAction                1
2017-07-31 15:47:09.428260 EDT | PolicyRegParamNorm          22.6137
2017-07-31 15:47:09.428500 EDT | QFunRegParamNorm            24.5168
2017-07-31 15:47:09.428743 EDT | -----------------------  -------------
2017-07-31 15:47:09.429161 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #28 | Training started
2017-07-31 15:47:40.189333 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #28 | Training finished
2017-07-31 15:47:40.190949 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #28 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:47:40.191301 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #28 | Collecting samples for evaluation
2017-07-31 15:47:59.522721 EDT | -----------------------  -------------
2017-07-31 15:47:59.523921 EDT | Oracle Interactions      10631
2017-07-31 15:47:59.524138 EDT | Agent Interactions       18369
2017-07-31 15:47:59.524318 EDT | Epoch                       28
2017-07-31 15:47:59.524483 EDT | Iteration                   28
2017-07-31 15:47:59.524653 EDT | AverageReturn                5.83348
2017-07-31 15:47:59.524802 EDT | StdReturn                    0.059481
2017-07-31 15:47:59.524960 EDT | MaxReturn                    6.00829
2017-07-31 15:47:59.525128 EDT | MinReturn                    5.68722
2017-07-31 15:47:59.525305 EDT | AverageEsReturn              8.0437
2017-07-31 15:47:59.525468 EDT | StdEsReturn                  2.69262
2017-07-31 15:47:59.525625 EDT | MaxEsReturn                 20.7854
2017-07-31 15:47:59.525774 EDT | MinEsReturn                  5.40352
2017-07-31 15:47:59.525926 EDT | AverageDiscountedReturn      5.67474
2017-07-31 15:47:59.526073 EDT | AverageQLoss                 0.0437707
2017-07-31 15:47:59.526250 EDT | AveragePolicySurr           -2.435
2017-07-31 15:47:59.526410 EDT | AverageQ                     4.22034
2017-07-31 15:47:59.526556 EDT | AverageAbsQ                  4.25609
2017-07-31 15:47:59.526712 EDT | AverageY                     4.22093
2017-07-31 15:47:59.526864 EDT | AverageAbsY                  4.25667
2017-07-31 15:47:59.527015 EDT | AverageAbsQYDiff             0.118103
2017-07-31 15:47:59.527171 EDT | AverageAction                1
2017-07-31 15:47:59.527315 EDT | PolicyRegParamNorm          22.8297
2017-07-31 15:47:59.527471 EDT | QFunRegParamNorm            24.9428
2017-07-31 15:47:59.527622 EDT | -----------------------  -------------
2017-07-31 15:47:59.527934 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #29 | Training started
2017-07-31 15:48:29.206181 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #29 | Training finished
2017-07-31 15:48:29.207080 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #29 | Trained qf 1000 steps, policy 1000 steps
2017-07-31 15:48:29.207399 EDT | [Active_RL/Hard_Agent_DDPG/Experiment_0] epoch #29 | Collecting samples for evaluation
2017-07-31 15:48:47.633161 EDT | -----------------------  -------------
2017-07-31 15:48:47.634133 EDT | Oracle Interactions      10677
2017-07-31 15:48:47.634359 EDT | Agent Interactions       19323
2017-07-31 15:48:47.634547 EDT | Epoch                       29
2017-07-31 15:48:47.634748 EDT | Iteration                   29
2017-07-31 15:48:47.634906 EDT | AverageReturn                5.83579
2017-07-31 15:48:47.635064 EDT | StdReturn                    0.0612149
2017-07-31 15:48:47.635229 EDT | MaxReturn                    6.00939
2017-07-31 15:48:47.635384 EDT | MinReturn                    5.70513
2017-07-31 15:48:47.635546 EDT | AverageEsReturn              7.78022
2017-07-31 15:48:47.635700 EDT | StdEsReturn                  5.6565
2017-07-31 15:48:47.635868 EDT | MaxEsReturn                 55.676
2017-07-31 15:48:47.636047 EDT | MinEsReturn                  1.84653
2017-07-31 15:48:47.636197 EDT | AverageDiscountedReturn      5.67696
2017-07-31 15:48:47.636489 EDT | AverageQLoss                 0.0466415
2017-07-31 15:48:47.636653 EDT | AveragePolicySurr           -2.42411
2017-07-31 15:48:47.636809 EDT | AverageQ                     4.19135
2017-07-31 15:48:47.636960 EDT | AverageAbsQ                  4.22792
2017-07-31 15:48:47.637107 EDT | AverageY                     4.19168
2017-07-31 15:48:47.637265 EDT | AverageAbsY                  4.22827
2017-07-31 15:48:47.637423 EDT | AverageAbsQYDiff             0.122469
2017-07-31 15:48:47.637577 EDT | AverageAction                1
2017-07-31 15:48:47.637736 EDT | PolicyRegParamNorm          22.9023
2017-07-31 15:48:47.637956 EDT | QFunRegParamNorm            25.3234
2017-07-31 15:48:47.638233 EDT | -----------------------  -------------
